{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建计算图\n",
    "修改：我们做风格转换不需要最后的fc，并且fc含有参数多，费时，所以我们这里注释掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image #图像处理库\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/bessie/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG中自带常量，VGG有将图片进行预处理，其中一个步骤是normalization:\n",
    "#减去image_net的RGB通道的各个均值\n",
    "VGG_MEAN = [103.939, 116.779, 123.68] #在vggnet的code中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNet:\n",
    "    \"\"\"Builds VGG-16 net structure,\n",
    "        load parameters from pre-trained models.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dict):\n",
    "        self.data_dict = data_dict\n",
    "        \n",
    "    def get_conv_filter(self, name): #抽取卷积参数\n",
    "        \"\"\"eg. conv1_1 = data_dict['conv1_1']\"\"\"\n",
    "        #tf.constant() #因为模型是预处理好的，所以我们不会改变参数，所以定义为常量。\n",
    "        #另一个方法：可以设置成trainable = False\n",
    "        return tf.constant(self.data_dict[name][0], name = 'conv') #这里应该是w,b中的w\n",
    "    \n",
    "    def get_fc_weight(self, name):\n",
    "        return tf.constant(self.data_dict[name][0], name = 'fc') #这里应该是w,b中的w\n",
    "    \n",
    "    def get_bias(self, name):\n",
    "        return tf.constant(self.data_dict[name][1], name = 'bias') #这里应该是w,b中的b\n",
    "    \n",
    "    #创建卷积层，池化层，全连接层\n",
    "    def conv_layer(self, x, name):\n",
    "        \"\"\"Builds convolution layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            #加上name_scope是命名规范：\n",
    "            #1. 防止命名冲突\n",
    "            #2. tensorboard打印名字更加清晰规范\n",
    "            conv_w = self.get_conv_filter(name)\n",
    "            conv_b = self.get_bias(name)\n",
    "            \n",
    "            #现在不再使用tf.layers.conv2d(),因为我们已经有了pre-trained的参数\n",
    "            #现在使用更基础的api: tf.nn.conv2d()\n",
    "            h = tf.nn.conv2d(x, conv_w, [1,1,1,1], padding = 'SAME') #x是input，[1,1,1,1]是strides步长，因为这里x是四维，所以我们输入四个数\n",
    "            h = tf.nn.bias_add(h, conv_b)\n",
    "            \n",
    "            #激活函数\n",
    "            h = tf.nn.relu(h)\n",
    "            return h\n",
    "        \n",
    "    #也是使用tf.nn.max_pool()而不是tf.layers.max_pooling2d\n",
    "    def pooling_layer(self, x, name):\n",
    "        \"\"\"Builds pooling layer.\"\"\"\n",
    "        return tf.nn.max_pool(x,\n",
    "                                 ksize = [1,2,2,1], #因为是按照长和宽来池化，所以是中间两个维度是2，其余维度是1\n",
    "                                 strides = [1,2,2,1], \n",
    "                                 padding = 'SAME', \n",
    "                                 name = name) \n",
    "    \n",
    "    \n",
    "    def fc_layer(self, x, name, activation = tf.nn.relu):\n",
    "        \"\"\"Builds fully-connected layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            fc_w = self.get_fc_weight(name)\n",
    "            fc_b = self.get_bias(name)\n",
    "            h = tf.matmul(x, fc_w) #让输入x与w进行操作\n",
    "            h = tf.nn.bias_add(h, fc_b)\n",
    "            if activation == None:\n",
    "                return h\n",
    "            else:\n",
    "                return activation(h)\n",
    "            \n",
    "    \n",
    "    #创建展平功能，展平后输入给全连接层：做的是reshape操作，我们需要知道reshape之后的size有多大\n",
    "    #展平之后，需要的长宽厚的乘积\n",
    "    def flatten_layer(self, x, name):\n",
    "        \"\"\"Builds flatten layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            #[batch_size, img_width, img_height, channel]\n",
    "            x_shape = x.get_shape().as_list()\n",
    "            dim = 1\n",
    "            for d in x_shape[1:]:\n",
    "                dim *= d\n",
    "            x = tf.reshape(x, [-1, dim]) #这里的-1，是reshape推断出来的，也是我们的batch_size, 你也可以写成[x_shape[0], dim]\n",
    "            return x\n",
    "       \n",
    "    #建立vgg\n",
    "    #我们现在就要做图像的风格转换，需要的图片只有一个，所以第一个维度是1\n",
    "    #vggnet的设置中，图像大小是224*224\n",
    "    def build(self, x_rgb):\n",
    "        \"\"\"BUild VGG16 network structure.\n",
    "        Args:\n",
    "        - x_rgb: eg. [1, 224, 224, 3]\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        print(\"Building model...\")\n",
    "        \n",
    "        #每个通道减去均值VGG_MEAN，先拆分通道\n",
    "        #复习：tf.split() 之前用于：深度可分离卷积，数据增强\n",
    "        r, g, b = tf.split(x_rgb, [1,1,1], axis = 3) #切分成三通道：[1,1,1]\n",
    "        \n",
    "        #去除均值后，需要合并。这里注意vggnet输入的通道顺序是BGR\n",
    "        #意味着之前写的VGG_MEAN的三个数分别是 BGR 的均值\n",
    "        x_bgr = tf.concat([b - VGG_MEAN[0], \n",
    "                           g - VGG_MEAN[1],\n",
    "                           r - VGG_MEAN[2]],\n",
    "                          axis = 3) #在第四个维度，channel上合并\n",
    "        \n",
    "        #预处理之后，判断一下我们的维度是 224*224*3\n",
    "        assert x_bgr.get_shape().as_list()[1:] == [224,224,3]\n",
    "        \n",
    "        #构建前两个卷积层：\n",
    "        #vgg16：\n",
    "        #第一个结构(stage)：两个卷积层 -> 一个池化层\n",
    "        #第二个结构：两个卷积层 -> 一个池化层\n",
    "        #第3个结构：3个卷积层 -> 一个池化层\n",
    "        #第4个结构：3个卷积层 -> 一个池化层\n",
    "        #第5个结构：3个卷积层 -> 一个池化层\n",
    "        #第6个结构：3个全连接层\n",
    "        #2*2 + 3*3 + 3 = 4 + 9 + 3 = 16, 也就是vgg16 \n",
    "        \n",
    "        ##注意：self.conv_layer(xx,yy)第二个参数的名字必须是data_dict.keys()中的\n",
    "        #dict_keys(['conv5_1', 'fc6', 'conv5_3', 'conv5_2', 'fc8', 'fc7', 'conv4_1', 'conv4_2', 'conv4_3', 'conv3_3', 'conv3_2', 'conv3_1', 'conv1_1', 'conv1_2', 'conv2_2', 'conv2_1'])\n",
    "        #我们将每一个层，设置成了成员变量, eg. self.conv1_1, 可能会用其中的某一层计算风格损失或者内容损失，设置成成员变量我们以后可以方便使用\n",
    "        self.conv1_1 = self.conv_layer(x_bgr, 'conv1_1')\n",
    "        self.conv1_2 = self.conv_layer(self.conv1_1, 'conv1_2')\n",
    "        self.pool1 = self.pooling_layer(self.conv1_2, 'pool1') #pool1因为不是data_dict里面预处理好的，所以我们可以随意命名, 例如pool1。并且可以不用将它设置成成员函数，只不过这里为了统一起见\n",
    "        \n",
    "        self.conv2_1 = self.conv_layer(self.pool1, 'conv2_1')\n",
    "        self.conv2_2 = self.conv_layer(self.conv2_1, 'conv2_2')\n",
    "        self.pool2 = self.pooling_layer(self.conv2_2, 'pool2')\n",
    "        \n",
    "        self.conv3_1 = self.conv_layer(self.pool2, 'conv3_1')\n",
    "        self.conv3_2 = self.conv_layer(self.conv3_1, 'conv3_2')\n",
    "        self.conv3_3 = self.conv_layer(self.conv3_2, 'conv3_3')\n",
    "        self.pool3 = self.pooling_layer(self.conv3_3, 'pool3')\n",
    "        \n",
    "        self.conv4_1 = self.conv_layer(self.pool3, 'conv4_1')\n",
    "        self.conv4_2 = self.conv_layer(self.conv4_1, 'conv4_2')\n",
    "        self.conv4_3 = self.conv_layer(self.conv4_2, 'conv4_3')\n",
    "        self.pool4 = self.pooling_layer(self.conv4_3, 'pool4')\n",
    "        \n",
    "        self.conv5_1 = self.conv_layer(self.pool4, 'conv5_1')\n",
    "        self.conv5_2 = self.conv_layer(self.conv5_1, 'conv5_2')\n",
    "        self.conv5_3 = self.conv_layer(self.conv5_2, 'conv5_3')\n",
    "        self.pool5 = self.pooling_layer(self.conv5_3, 'pool5')\n",
    "        \n",
    "        '''\n",
    "        #展开 -> 全连接\n",
    "        self.flatten5 = self.flatten_layer(self.pool5, 'flatten')\n",
    "        self.fc6 = self.fc_layer(self.flatten5, 'fc6')\n",
    "        self.fc7 = self.fc_layer(self.fc6, 'fc7')\n",
    "        \n",
    "        #最后的fc8输出1k个值，给softmax()去计算概率分布\n",
    "        #所以fc8不需要activation\n",
    "        self.fc8 = self.fc_layer(self.fc7, 'fc8', activation = None)\n",
    "        \n",
    "        #计算softmax\n",
    "        self.prob = tf.nn.softmax(self.fc8, name = 'prob')\n",
    "        '''\n",
    "        \n",
    "        print(\"Building model finished: %4ds\" % (time.time() - start_time))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试我们的VGGNet 类是否成功："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Building model finished:    0s\n"
     ]
    }
   ],
   "source": [
    "#加载vgg16\n",
    "vgg16_npy_path = '../../../other_datasets/vgg16.npy'\n",
    "data_dict = np.load(vgg16_npy_path, encoding = 'latin1').item() #加item()是为了创建成字典\n",
    "\n",
    "vgg16_for_result = VGGNet(data_dict)\n",
    "content = tf.placeholder(tf.float32, shape = [1,224,224,3])\n",
    "vgg16_for_result.build(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将图片都设置为大小是 224 * 224 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 224, 224, 3)\n",
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "content_img_path = './others/citi.JPG'\n",
    "\n",
    "img_string = tf.read_file(content_img_path)\n",
    "img_decoded = tf.image.decode_image(img_string)\n",
    "\n",
    "sess = tf.Session()\n",
    "#用sess执行这个图\n",
    "img_decoded_val = sess.run(img_decoded)\n",
    "\n",
    "#因为tf.image.resize_bicubic()处理的是batch数据，所以需要4维输入，第一维就是图片数\n",
    "shape = img_decoded_val.shape\n",
    "img_decoded = tf.reshape(img_decoded, [1, shape[0], shape[1], shape[2]])\n",
    "\n",
    "#在img_decoded上面加工：\n",
    "resize_img = tf.image.resize_bicubic(img_decoded, [224, 224])\n",
    "\n",
    "sess = tf.Session()\n",
    "img_decoded_val = sess.run(resize_img)\n",
    "\n",
    "#需要将4维改成3维：\n",
    "print(img_decoded_val.shape)#(1, 224, 224, 3)\n",
    "img_decoded_val = img_decoded_val.reshape((224, 224, 3))\n",
    "print(img_decoded_val.shape)\n",
    "\n",
    "#需要做类型变换：从[0,1]之间的小数float 改为 [0,255]integer \n",
    "img_decoded_val = np.asarray(img_decoded_val, np.uint8)\n",
    "\n",
    "resize_img_path = os.path.join('./others', 'resized_citi.jpg')\n",
    "\n",
    "img = Image.fromarray(img_decoded_val) #可以将numpy数组转换成图片\n",
    "img.save(resize_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 224, 224, 3)\n",
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "style_img_path = './others/starry_night.jpeg'\n",
    "\n",
    "img_string = tf.read_file(style_img_path)\n",
    "img_decoded = tf.image.decode_image(img_string)\n",
    "\n",
    "sess = tf.Session()\n",
    "#用sess执行这个图\n",
    "img_decoded_val = sess.run(img_decoded)\n",
    "\n",
    "#因为tf.image.resize_bicubic()处理的是batch数据，所以需要4维输入，第一维就是图片数\n",
    "shape = img_decoded_val.shape\n",
    "img_decoded = tf.reshape(img_decoded, [1, shape[0], shape[1], shape[2]])\n",
    "\n",
    "#在img_decoded上面加工：\n",
    "resize_img = tf.image.resize_bicubic(img_decoded, [224, 224])\n",
    "\n",
    "sess = tf.Session()\n",
    "img_decoded_val = sess.run(resize_img)\n",
    "\n",
    "#需要将4维改成3维：\n",
    "print(img_decoded_val.shape)##(1, 224, 224, 3)\n",
    "img_decoded_val = img_decoded_val.reshape((224, 224, 3))\n",
    "print(img_decoded_val.shape)\n",
    "\n",
    "#需要做类型变换：从[0,1]之间的小数float 改为 [0,255]integer \n",
    "img_decoded_val = np.asarray(img_decoded_val, np.uint8)\n",
    "\n",
    "resize_img_path = os.path.join('./others', 'resized_starry_night.jpg')\n",
    "\n",
    "img = Image.fromarray(img_decoded_val) #可以将numpy数组转换成图片\n",
    "img.save(resize_img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始风格转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_img_path = './others/resized_citi.JPG'\n",
    "style_img_path = './others/resized_starry_night.jpg'\n",
    "\n",
    "#训练次数\n",
    "num_steps = 100\n",
    "\n",
    "#学习率\n",
    "learning_rate = 10\n",
    "\n",
    "#风格系数，内容系数\n",
    "lambda_c = 0.1 #内容\n",
    "lambda_s = 5000 #风格\n",
    "\n",
    "#输出文件夹, 每一步都有新图像，将所有图像输入这个文件夹\n",
    "output_dir = './001_style_transfer_output'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Building model finished:    0s\n",
      "Building model...\n",
      "Building model finished:    0s\n",
      "Building model...\n",
      "Building model finished:    0s\n"
     ]
    }
   ],
   "source": [
    "#定义随机的初始图片：\n",
    "def initial_result(shape, mean, stddev):\n",
    "    #用正态分布初始化\n",
    "    initial = tf.truncated_normal(shape, mean = mean, stddev = stddev) #截断的正态分布\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#将风格，内容图像读取进来：\n",
    "def read_img(img_name):\n",
    "    #PIL里面的函数,读取img\n",
    "    img = Image.open(img_name) \n",
    "    \n",
    "    #变成numpy的矩阵\n",
    "    np_img = np.array(img) #（224,224,3)\n",
    "    \n",
    "    #矩阵是 (224, 224, 3), 需要改变成4维：\n",
    "    #因为只有一个样本，所以可以不用reshape，而是直接用[np_img]，将np_img包含在一个list [] 里面\n",
    "    #如此就直接多了一个维度，变成(1, 224, 224, 3)\n",
    "    np_img = np.asarray([np_img], dtype = np.int32)\n",
    "    \n",
    "    return np_img\n",
    "\n",
    "#计算Gram矩阵，用于风格损失：\n",
    "def gram_matrix(x):\n",
    "    \"\"\"Calculates gram matrix\n",
    "    Args:\n",
    "    - x: features extracted from VGG Net. shape: [1, width, height, channel]\n",
    "    \"\"\"\n",
    "    batch, w, h, ch = x.get_shape().as_list()\n",
    "    features = tf.reshape(x, [batch, h*w, ch]) #将w，h合成一个维度\n",
    "    \n",
    "    #针对channel，两两进行计算余弦相似度，得到gram matrix\n",
    "    #1. 我们的features的第一个维度batch，永远都等于1\n",
    "    #2. 我们的features的后两个维度，我们可以看成一个二维矩阵，假设叫A：行是h*w, 列是ch\n",
    "    #3. 计算所有channel两两之间的相似度，我们可以看做将A矩阵中，抽取两个列进行点乘\n",
    "    #4. 假设有k个channel，我们最后的gram matrix是k * k 矩阵，其中第i行，第j列代表第i个ch和第j个ch的相似度\n",
    "    #5. 计算ch * ch的点乘，可以用 (ch * hw) * (hw * ch)进行点乘，也就是features的转置，点乘features\n",
    "    #6. 矩阵乘法用tf.matmul(), 其中adjoin_a = True表示第一个参数要转置\n",
    "    #7. 担心点乘后值过大，所以我们还会除以一个常数：也就是各个维度的乘积，例如224*224*3\n",
    "    gram = tf.matmul(features, features, adjoint_a = True) \\\n",
    "            / tf.constant(ch * w * h, tf.float32)\n",
    "    return gram\n",
    "\n",
    "#初始化图像: mean是[0,255]的中间值\n",
    "result = initial_result((1, 224, 224, 3), 127.5, 20)\n",
    "\n",
    "#读取风格，内容图片\n",
    "content_val = read_img(content_img_path)\n",
    "style_val = read_img(style_img_path)\n",
    "\n",
    "#用feed_dict塞入，所以我们需要先创建placeholder\n",
    "content = tf.placeholder(tf.float32, shape = [1,224,224,3])\n",
    "style = tf.placeholder(tf.float32, shape = [1,224,224,3])\n",
    "\n",
    "#将三张图输入vggnet，然后提取特征\n",
    "data_dict = np.load(vgg16_npy_path, encoding = 'latin1').item()\n",
    "\n",
    "#创建三个vggnet，都是同样的参数\n",
    "#给内容图像创建vggnet\n",
    "vgg_for_content = VGGNet(data_dict)\n",
    "\n",
    "#给风格图像创建vggnet\n",
    "vgg_for_style = VGGNet(data_dict)\n",
    "\n",
    "#给结果图像创建vggnet\n",
    "vgg_for_result = VGGNet(data_dict)\n",
    "\n",
    "#调用build函数，完成vggnet构建\n",
    "#content和vgg_for_content进行关联\n",
    "vgg_for_content.build(content)\n",
    "vgg_for_style.build(style)\n",
    "vgg_for_result.build(result)\n",
    "\n",
    "#vggnet的每个层都可以进行特征提取\n",
    "#content：越底层提取的特征越清晰\n",
    "#以下是提取了5个特征，我们先注释掉后3个\n",
    "content_features = [\n",
    "    vgg_for_content.conv1_2,\n",
    "    vgg_for_content.conv2_2,\n",
    "    #vgg_for_content.conv3_3,\n",
    "    #vgg_for_content.conv4_3,\n",
    "    #vgg_for_content.conv5_3,\n",
    "]\n",
    "\n",
    "#给内容图像提取了哪些特征，就要对结果图像提取哪些特征\n",
    "result_content_features = [\n",
    "    vgg_for_result.conv1_2,\n",
    "    vgg_for_result.conv2_2,\n",
    "    #vgg_for_result.conv3_3,\n",
    "    #vgg_for_result.conv4_3,\n",
    "    #vgg_for_result.conv5_3,\n",
    "]\n",
    "\n",
    "#style: 越高层越好，所以注释掉底层\n",
    "style_features = [\n",
    "    #vgg_for_style.conv1_2,\n",
    "    #vgg_for_style.conv2_2,\n",
    "    #vgg_for_style.conv3_3,\n",
    "    vgg_for_style.conv4_3,\n",
    "    #vgg_for_style.conv5_3,\n",
    "]\n",
    "\n",
    "#给风格图像的风格特征计算gram矩阵：\n",
    "style_gram = [gram_matrix(feature) for feature in style_features]\n",
    "\n",
    "#给风格图像提取了哪些特征，就要对结果图像提取哪些特征\n",
    "result_style_features = [\n",
    "    #vgg_for_result.conv1_2,\n",
    "    #vgg_for_result.conv2_2,\n",
    "    #vgg_for_result.conv3_3,\n",
    "    vgg_for_result.conv4_3,\n",
    "    #vgg_for_result.conv5_3,\n",
    "]\n",
    "\n",
    "#给结果图像的风格特征计算gram矩阵：\n",
    "result_style_gram = [gram_matrix(feature) for feature in result_style_features]\n",
    "\n",
    "#计算内容损失 + 风格损失\n",
    "#1. 内容损失：是每一层损失的加和\n",
    "#zip: 将两个数组绑定在一起\n",
    "#例如：[1,2], [3,4] \n",
    "#zip([1,2], [3,4]) -> [(1,3), (2,4)]\n",
    "#两个list -> 一个list中两个pair：(1,3) 和 (2,4)\n",
    "#想象：将两个list竖着放，然后横着拿出来\n",
    "content_loss = tf.zeros(1, tf.float32) #一个数，是标量\n",
    "for c, c_ in zip(content_features, result_content_features):\n",
    "    #c, c_ = (content_features[0], result_content_features[0])\n",
    "    #c, c_ = (content_features[1], result_content_features[1])\n",
    "    #内容损失是提取的特征：c, c_的平方差，再求平均。\n",
    "    #平均是长宽高所有维度的平均，因为shape = [1, width, height, channel], 所以axis = [1,2,3]\n",
    "    content_loss += tf.reduce_mean((c - c_) ** 2, [1,2,3])\n",
    "    \n",
    "#2. 风格损失\n",
    "#将某一层的特征提取出来，会得到feature_map\n",
    "style_loss = tf.zeros(1, tf.float32)\n",
    "for s, s_ in zip(style_gram, result_style_gram):\n",
    "    #平方差损失函数，在1，2维度求均值。因为gram_matrix()已经将width和height降成1维。所以现在第零维是batch，第一维是w*h, 第二维度是ch\n",
    "    style_loss += tf.reduce_mean((s - s_) ** 2, [1,2])\n",
    "\n",
    "#loss加权\n",
    "loss = content_loss * lambda_c + style_loss * lambda_s\n",
    "    \n",
    "#给损失函数计算梯度\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 图像风格转换的训练流程图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1, loss_value: 66825.5000, content_loss: 349478.4375, style_loss:   6.3755\n",
      "step: 2, loss_value: 53740.0391, content_loss: 281181.2188, style_loss:   5.1244\n",
      "step: 3, loss_value: 49576.6016, content_loss: 248774.3750, style_loss:   4.9398\n",
      "step: 4, loss_value: 44447.9062, content_loss: 241145.2812, style_loss:   4.0667\n",
      "step: 5, loss_value: 39966.3281, content_loss: 240859.6719, style_loss:   3.1761\n",
      "step: 6, loss_value: 38938.0703, content_loss: 244702.5469, style_loss:   2.8936\n",
      "step: 7, loss_value: 35734.5195, content_loss: 247492.1250, style_loss:   2.1971\n",
      "step: 8, loss_value: 34325.6172, content_loss: 249424.0156, style_loss:   1.8766\n",
      "step: 9, loss_value: 33422.2383, content_loss: 249866.9375, style_loss:   1.6871\n",
      "step: 10, loss_value: 31979.2852, content_loss: 247504.4531, style_loss:   1.4458\n",
      "step: 11, loss_value: 31009.2090, content_loss: 243408.1250, style_loss:   1.3337\n",
      "step: 12, loss_value: 29961.8125, content_loss: 237844.6250, style_loss:   1.2355\n",
      "step: 13, loss_value: 28822.1191, content_loss: 230505.2500, style_loss:   1.1543\n",
      "step: 14, loss_value: 27702.5781, content_loss: 222263.0469, style_loss:   1.0953\n",
      "step: 15, loss_value: 26571.7773, content_loss: 213662.7188, style_loss:   1.0411\n",
      "step: 16, loss_value: 25452.3828, content_loss: 204505.5938, style_loss:   1.0004\n",
      "step: 17, loss_value: 24405.7207, content_loss: 195268.1875, style_loss:   0.9758\n",
      "step: 18, loss_value: 23368.0762, content_loss: 186214.6250, style_loss:   0.9493\n",
      "step: 19, loss_value: 22355.9375, content_loss: 177285.6875, style_loss:   0.9255\n",
      "step: 20, loss_value: 21390.3066, content_loss: 168903.5000, style_loss:   0.9000\n",
      "step: 21, loss_value: 20476.5176, content_loss: 161034.2656, style_loss:   0.8746\n",
      "step: 22, loss_value: 19622.3633, content_loss: 153695.1719, style_loss:   0.8506\n",
      "step: 23, loss_value: 18827.0078, content_loss: 146932.8594, style_loss:   0.8267\n",
      "step: 24, loss_value: 18207.0547, content_loss: 140495.7969, style_loss:   0.8315\n",
      "step: 25, loss_value: 17671.0664, content_loss: 134742.5625, style_loss:   0.8394\n",
      "step: 26, loss_value: 17398.5000, content_loss: 128949.0156, style_loss:   0.9007\n",
      "step: 27, loss_value: 16470.8320, content_loss: 124152.1562, style_loss:   0.8111\n",
      "step: 28, loss_value: 15879.4600, content_loss: 119788.7578, style_loss:   0.7801\n",
      "step: 29, loss_value: 15510.0039, content_loss: 115424.2344, style_loss:   0.7935\n",
      "step: 30, loss_value: 15104.4043, content_loss: 111600.6719, style_loss:   0.7889\n",
      "step: 31, loss_value: 14691.2793, content_loss: 107876.8125, style_loss:   0.7807\n",
      "step: 32, loss_value: 14101.9121, content_loss: 104444.0000, style_loss:   0.7315\n",
      "step: 33, loss_value: 13757.9531, content_loss: 100908.9297, style_loss:   0.7334\n",
      "step: 34, loss_value: 13405.6895, content_loss: 97639.1172, style_loss:   0.7284\n",
      "step: 35, loss_value: 13153.0898, content_loss: 94650.0703, style_loss:   0.7376\n",
      "step: 36, loss_value: 13208.8506, content_loss: 91567.0781, style_loss:   0.8104\n",
      "step: 37, loss_value: 12650.3145, content_loss: 89495.8281, style_loss:   0.7401\n",
      "step: 38, loss_value: 12432.2705, content_loss: 87449.6406, style_loss:   0.7375\n",
      "step: 39, loss_value: 12030.3594, content_loss: 85209.2422, style_loss:   0.7019\n",
      "step: 40, loss_value: 11823.8496, content_loss: 83133.6172, style_loss:   0.7021\n",
      "step: 41, loss_value: 11719.9219, content_loss: 81147.2031, style_loss:   0.7210\n",
      "step: 42, loss_value: 11458.0195, content_loss: 79293.6562, style_loss:   0.7057\n",
      "step: 43, loss_value: 11521.9951, content_loss: 77179.5000, style_loss:   0.7608\n",
      "step: 44, loss_value: 11101.7578, content_loss: 76044.4844, style_loss:   0.6995\n",
      "step: 45, loss_value: 10928.2949, content_loss: 74894.4844, style_loss:   0.6878\n",
      "step: 46, loss_value: 10620.0205, content_loss: 73356.2500, style_loss:   0.6569\n",
      "step: 47, loss_value: 10539.0996, content_loss: 71978.1953, style_loss:   0.6683\n",
      "step: 48, loss_value: 10392.9717, content_loss: 70569.5781, style_loss:   0.6672\n",
      "step: 49, loss_value: 10407.6611, content_loss: 69286.0312, style_loss:   0.6958\n",
      "step: 50, loss_value: 10679.1455, content_loss: 67540.3594, style_loss:   0.7850\n",
      "step: 51, loss_value: 10241.3525, content_loss: 66656.9688, style_loss:   0.7151\n",
      "step: 52, loss_value: 9993.4648, content_loss: 66023.0859, style_loss:   0.6782\n",
      "step: 53, loss_value: 9800.6475, content_loss: 65164.2891, style_loss:   0.6568\n",
      "step: 54, loss_value: 9757.0498, content_loss: 64130.7461, style_loss:   0.6688\n",
      "step: 55, loss_value: 9587.7666, content_loss: 63133.8320, style_loss:   0.6549\n",
      "step: 56, loss_value: 9508.2900, content_loss: 62340.3594, style_loss:   0.6549\n",
      "step: 57, loss_value: 9429.6680, content_loss: 61198.8203, style_loss:   0.6620\n",
      "step: 58, loss_value: 9301.2344, content_loss: 60579.8594, style_loss:   0.6486\n",
      "step: 59, loss_value: 9179.8594, content_loss: 60001.6680, style_loss:   0.6359\n",
      "step: 60, loss_value: 9068.0830, content_loss: 59224.4766, style_loss:   0.6291\n",
      "step: 61, loss_value: 9094.5293, content_loss: 58194.7266, style_loss:   0.6550\n",
      "step: 62, loss_value: 8972.5410, content_loss: 57683.4766, style_loss:   0.6408\n",
      "step: 63, loss_value: 9118.6787, content_loss: 56920.6328, style_loss:   0.6853\n",
      "step: 64, loss_value: 8804.3701, content_loss: 56441.7383, style_loss:   0.6320\n",
      "step: 65, loss_value: 8752.6172, content_loss: 55480.0391, style_loss:   0.6409\n",
      "step: 66, loss_value: 8518.7910, content_loss: 54845.1992, style_loss:   0.6069\n",
      "step: 67, loss_value: 8519.7715, content_loss: 54295.2148, style_loss:   0.6180\n",
      "step: 68, loss_value: 8490.7148, content_loss: 53413.1445, style_loss:   0.6299\n",
      "step: 69, loss_value: 8560.2227, content_loss: 53017.3516, style_loss:   0.6517\n",
      "step: 70, loss_value: 8871.1543, content_loss: 52452.9688, style_loss:   0.7252\n",
      "step: 71, loss_value: 8576.0352, content_loss: 52319.4688, style_loss:   0.6688\n",
      "step: 72, loss_value: 8511.4922, content_loss: 52098.5312, style_loss:   0.6603\n",
      "step: 73, loss_value: 8289.9316, content_loss: 52200.6133, style_loss:   0.6140\n",
      "step: 74, loss_value: 8337.7559, content_loss: 51952.9297, style_loss:   0.6285\n",
      "step: 75, loss_value: 8236.5312, content_loss: 51507.6641, style_loss:   0.6172\n",
      "step: 76, loss_value: 8375.1924, content_loss: 51273.9922, style_loss:   0.6496\n",
      "step: 77, loss_value: 8777.1494, content_loss: 50663.0898, style_loss:   0.7422\n",
      "step: 78, loss_value: 8583.4766, content_loss: 51234.8359, style_loss:   0.6920\n",
      "step: 79, loss_value: 8546.0947, content_loss: 51297.8125, style_loss:   0.6833\n",
      "step: 80, loss_value: 8346.8535, content_loss: 51613.2656, style_loss:   0.6371\n",
      "step: 81, loss_value: 8405.1113, content_loss: 52079.1484, style_loss:   0.6394\n",
      "step: 82, loss_value: 8479.3613, content_loss: 51908.9375, style_loss:   0.6577\n",
      "step: 83, loss_value: 8360.7373, content_loss: 51789.1250, style_loss:   0.6364\n",
      "step: 84, loss_value: 8263.6562, content_loss: 51322.0547, style_loss:   0.6263\n",
      "step: 85, loss_value: 8077.8711, content_loss: 50880.5312, style_loss:   0.5980\n",
      "step: 86, loss_value: 8020.5635, content_loss: 50244.0117, style_loss:   0.5992\n",
      "step: 87, loss_value: 7961.7407, content_loss: 49607.1211, style_loss:   0.6002\n",
      "step: 88, loss_value: 7886.0044, content_loss: 48783.8672, style_loss:   0.6015\n",
      "step: 89, loss_value: 8009.4443, content_loss: 47965.6602, style_loss:   0.6426\n",
      "step: 90, loss_value: 8130.2061, content_loss: 48043.5391, style_loss:   0.6652\n",
      "step: 91, loss_value: 8576.1523, content_loss: 47105.4023, style_loss:   0.7731\n",
      "step: 92, loss_value: 7993.7749, content_loss: 47163.0391, style_loss:   0.6555\n",
      "step: 93, loss_value: 8049.8579, content_loss: 48056.4180, style_loss:   0.6488\n",
      "step: 94, loss_value: 8098.1729, content_loss: 47902.7188, style_loss:   0.6616\n",
      "step: 95, loss_value: 8129.9023, content_loss: 47835.2695, style_loss:   0.6693\n",
      "step: 96, loss_value: 8022.6089, content_loss: 48245.7969, style_loss:   0.6396\n",
      "step: 97, loss_value: 7910.6064, content_loss: 48298.3945, style_loss:   0.6162\n",
      "step: 98, loss_value: 7973.3164, content_loss: 48033.4883, style_loss:   0.6340\n",
      "step: 99, loss_value: 7829.2271, content_loss: 47859.0898, style_loss:   0.6087\n",
      "step: 100, loss_value: 7875.7329, content_loss: 47973.3750, style_loss:   0.6157\n"
     ]
    }
   ],
   "source": [
    "#定义初始化op\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op) #初始化变量，在图像风格转换中，变量只有一个：结果图像（对结果图像进行梯度下降）\n",
    "    \n",
    "    #开始训练：\n",
    "    for step in range(num_steps):\n",
    "        #最后一个_是因为要训练\n",
    "        loss_value, content_loss_value, style_loss_value, _ \\\n",
    "            = sess.run([loss, content_loss, style_loss, train_op],\n",
    "                        feed_dict = {\n",
    "                            content: content_val,\n",
    "                            style: style_val,\n",
    "                        })\n",
    "        print('step: %d, loss_value: %8.4f, content_loss: %8.4f, style_loss: %8.4f' \\\n",
    "             % (step+1, \n",
    "                loss_value[0],  #之所以是[0], 是因为我们是四维的，第一个维度只有一个元素，所以取[0]\n",
    "                content_loss_value[0], \n",
    "                style_loss_value[0]))\n",
    "        \n",
    "        #获得每一步的结果图像，输出到dir中\n",
    "        result_img_path = os.path.join(\n",
    "            output_dir, 'result-%05d.jpg' % (step + 1))\n",
    "        \n",
    "        #将参数值，也就是结果图像的变量值，取出来\n",
    "        #result是之前定义的随机噪声 result = initial_result((1, 224, 224, 3), 127.5, 20)\n",
    "        result_val = result.eval(sess)[0] #因为第一个维度是只有一个元素，所以取出用[0]\n",
    "        result_val = np.clip(result_val, 0, 255) #clip的作用：裁剪，将小于0的设成0，大于255的设置成255\n",
    "\n",
    "        #之前是float，现在转换成int\n",
    "        img_arr = np.asarray(result_val, np.uint8) \n",
    "        \n",
    "        #用PIL转换成图片\n",
    "        img = Image.fromarray(img_arr) #可以将numpy数组转换成图片\n",
    "        img.save(result_img_path)\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为何之前设置这样的风格系数，内容系数\n",
    "1. 首先我们的style loss的计算中，gram matrix需要除以一个比较大的数tf.constant(ch * w * h, tf.float32)，导致了style loss比较小\n",
    "2. gram = tf.matmul(features, features, adjoint_a = True) \\\n",
    "            / tf.constant(ch * w * h, tf.float32)\n",
    "3. 既然style loss小，content loss大，为了均衡两者，所以一个乘500，一个乘0.1\n",
    "4. lambda_c = 0.1 #内容， lambda_s = 500 #风格\n",
    "5. style loss: 1.7 * 500 = 850\n",
    "6. content loss: 12997 * 0.1 = 1299"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 这个文件的泛化能力\n",
    "1. 首先可以通过注释和取消注释来选取不同层的特征：\n",
    "\n",
    "style_features = [\n",
    "    #vgg_for_style.conv1_2,\n",
    "    #vgg_for_style.conv2_2,\n",
    "    #vgg_for_style.conv3_3,\n",
    "    vgg_for_style.conv4_3,\n",
    "    #vgg_for_style.conv5_3,\n",
    "]\n",
    "\n",
    "2. 可以只要重建图片，或者只重建风格：\n",
    "\n",
    "lambda_c = 0 #只重建图片\n",
    "\n",
    "lambda_s = 5000 #只重建风格\n",
    "\n",
    "3. 可以为style loss 和 content loss的不同层添加系数：\n",
    "\n",
    "for c, c_ in zip(content_features, result_content_features):\n",
    "\n",
    "    content_loss += tf.reduce_mean((c - c_) ** 2, [1,2,3])\n",
    "    \n",
    "for s, s_ in zip(style_gram, result_style_gram):\n",
    "\n",
    "    style_loss += tf.reduce_mean((s - s_) ** 2, [1,2])\n",
    "    \n",
    "例如第一层的c和c_可以乘系数0.1\n",
    "\n",
    "content_loss += 0.1 * tf.reduce_mean((c - c_) ** 2, [1,2,3])\n",
    "\n",
    "例如第二层的c和c_可以乘系数0.5\n",
    "\n",
    "content_loss += 0.5 * tf.reduce_mean((c - c_) ** 2, [1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
