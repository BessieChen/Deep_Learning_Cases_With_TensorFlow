{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用深层网络进行内容重建\n",
    "效果很不好，根本看不清"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建计算图\n",
    "修改：我们做风格转换不需要最后的fc，并且fc含有参数多，费时，所以我们这里注释掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image #图像处理库\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/bessie/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG中自带常量，VGG有将图片进行预处理，其中一个步骤是normalization:\n",
    "#减去image_net的RGB通道的各个均值\n",
    "VGG_MEAN = [103.939, 116.779, 123.68] #在vggnet的code中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNet:\n",
    "    \"\"\"Builds VGG-16 net structure,\n",
    "        load parameters from pre-trained models.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dict):\n",
    "        self.data_dict = data_dict\n",
    "        \n",
    "    def get_conv_filter(self, name): #抽取卷积参数\n",
    "        \"\"\"eg. conv1_1 = data_dict['conv1_1']\"\"\"\n",
    "        #tf.constant() #因为模型是预处理好的，所以我们不会改变参数，所以定义为常量。\n",
    "        #另一个方法：可以设置成trainable = False\n",
    "        return tf.constant(self.data_dict[name][0], name = 'conv') #这里应该是w,b中的w\n",
    "    \n",
    "    def get_fc_weight(self, name):\n",
    "        return tf.constant(self.data_dict[name][0], name = 'fc') #这里应该是w,b中的w\n",
    "    \n",
    "    def get_bias(self, name):\n",
    "        return tf.constant(self.data_dict[name][1], name = 'bias') #这里应该是w,b中的b\n",
    "    \n",
    "    #创建卷积层，池化层，全连接层\n",
    "    def conv_layer(self, x, name):\n",
    "        \"\"\"Builds convolution layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            #加上name_scope是命名规范：\n",
    "            #1. 防止命名冲突\n",
    "            #2. tensorboard打印名字更加清晰规范\n",
    "            conv_w = self.get_conv_filter(name)\n",
    "            conv_b = self.get_bias(name)\n",
    "            \n",
    "            #现在不再使用tf.layers.conv2d(),因为我们已经有了pre-trained的参数\n",
    "            #现在使用更基础的api: tf.nn.conv2d()\n",
    "            h = tf.nn.conv2d(x, conv_w, [1,1,1,1], padding = 'SAME') #x是input，[1,1,1,1]是strides步长，因为这里x是四维，所以我们输入四个数\n",
    "            h = tf.nn.bias_add(h, conv_b)\n",
    "            \n",
    "            #激活函数\n",
    "            h = tf.nn.relu(h)\n",
    "            return h\n",
    "        \n",
    "    #也是使用tf.nn.max_pool()而不是tf.layers.max_pooling2d\n",
    "    def pooling_layer(self, x, name):\n",
    "        \"\"\"Builds pooling layer.\"\"\"\n",
    "        return tf.nn.max_pool(x,\n",
    "                                 ksize = [1,2,2,1], #因为是按照长和宽来池化，所以是中间两个维度是2，其余维度是1\n",
    "                                 strides = [1,2,2,1], \n",
    "                                 padding = 'SAME', \n",
    "                                 name = name) \n",
    "    \n",
    "    \n",
    "    def fc_layer(self, x, name, activation = tf.nn.relu):\n",
    "        \"\"\"Builds fully-connected layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            fc_w = self.get_fc_weight(name)\n",
    "            fc_b = self.get_bias(name)\n",
    "            h = tf.matmul(x, fc_w) #让输入x与w进行操作\n",
    "            h = tf.nn.bias_add(h, fc_b)\n",
    "            if activation == None:\n",
    "                return h\n",
    "            else:\n",
    "                return activation(h)\n",
    "            \n",
    "    \n",
    "    #创建展平功能，展平后输入给全连接层：做的是reshape操作，我们需要知道reshape之后的size有多大\n",
    "    #展平之后，需要的长宽厚的乘积\n",
    "    def flatten_layer(self, x, name):\n",
    "        \"\"\"Builds flatten layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            #[batch_size, img_width, img_height, channel]\n",
    "            x_shape = x.get_shape().as_list()\n",
    "            dim = 1\n",
    "            for d in x_shape[1:]:\n",
    "                dim *= d\n",
    "            x = tf.reshape(x, [-1, dim]) #这里的-1，是reshape推断出来的，也是我们的batch_size, 你也可以写成[x_shape[0], dim]\n",
    "            return x\n",
    "       \n",
    "    #建立vgg\n",
    "    #我们现在就要做图像的风格转换，需要的图片只有一个，所以第一个维度是1\n",
    "    #vggnet的设置中，图像大小是224*224\n",
    "    def build(self, x_rgb):\n",
    "        \"\"\"BUild VGG16 network structure.\n",
    "        Args:\n",
    "        - x_rgb: eg. [1, 224, 224, 3]\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        print(\"Building model...\")\n",
    "        \n",
    "        #每个通道减去均值VGG_MEAN，先拆分通道\n",
    "        #复习：tf.split() 之前用于：深度可分离卷积，数据增强\n",
    "        r, g, b = tf.split(x_rgb, [1,1,1], axis = 3) #切分成三通道：[1,1,1]\n",
    "        \n",
    "        #去除均值后，需要合并。这里注意vggnet输入的通道顺序是BGR\n",
    "        #意味着之前写的VGG_MEAN的三个数分别是 BGR 的均值\n",
    "        x_bgr = tf.concat([b - VGG_MEAN[0], \n",
    "                           g - VGG_MEAN[1],\n",
    "                           r - VGG_MEAN[2]],\n",
    "                          axis = 3) #在第四个维度，channel上合并\n",
    "        \n",
    "        #预处理之后，判断一下我们的维度是 224*224*3\n",
    "        assert x_bgr.get_shape().as_list()[1:] == [224,224,3]\n",
    "        \n",
    "        #构建前两个卷积层：\n",
    "        #vgg16：\n",
    "        #第一个结构(stage)：两个卷积层 -> 一个池化层\n",
    "        #第二个结构：两个卷积层 -> 一个池化层\n",
    "        #第3个结构：3个卷积层 -> 一个池化层\n",
    "        #第4个结构：3个卷积层 -> 一个池化层\n",
    "        #第5个结构：3个卷积层 -> 一个池化层\n",
    "        #第6个结构：3个全连接层\n",
    "        #2*2 + 3*3 + 3 = 4 + 9 + 3 = 16, 也就是vgg16 \n",
    "        \n",
    "        ##注意：self.conv_layer(xx,yy)第二个参数的名字必须是data_dict.keys()中的\n",
    "        #dict_keys(['conv5_1', 'fc6', 'conv5_3', 'conv5_2', 'fc8', 'fc7', 'conv4_1', 'conv4_2', 'conv4_3', 'conv3_3', 'conv3_2', 'conv3_1', 'conv1_1', 'conv1_2', 'conv2_2', 'conv2_1'])\n",
    "        #我们将每一个层，设置成了成员变量, eg. self.conv1_1, 可能会用其中的某一层计算风格损失或者内容损失，设置成成员变量我们以后可以方便使用\n",
    "        self.conv1_1 = self.conv_layer(x_bgr, 'conv1_1')\n",
    "        self.conv1_2 = self.conv_layer(self.conv1_1, 'conv1_2')\n",
    "        self.pool1 = self.pooling_layer(self.conv1_2, 'pool1') #pool1因为不是data_dict里面预处理好的，所以我们可以随意命名, 例如pool1。并且可以不用将它设置成成员函数，只不过这里为了统一起见\n",
    "        \n",
    "        self.conv2_1 = self.conv_layer(self.pool1, 'conv2_1')\n",
    "        self.conv2_2 = self.conv_layer(self.conv2_1, 'conv2_2')\n",
    "        self.pool2 = self.pooling_layer(self.conv2_2, 'pool2')\n",
    "        \n",
    "        self.conv3_1 = self.conv_layer(self.pool2, 'conv3_1')\n",
    "        self.conv3_2 = self.conv_layer(self.conv3_1, 'conv3_2')\n",
    "        self.conv3_3 = self.conv_layer(self.conv3_2, 'conv3_3')\n",
    "        self.pool3 = self.pooling_layer(self.conv3_3, 'pool3')\n",
    "        \n",
    "        self.conv4_1 = self.conv_layer(self.pool3, 'conv4_1')\n",
    "        self.conv4_2 = self.conv_layer(self.conv4_1, 'conv4_2')\n",
    "        self.conv4_3 = self.conv_layer(self.conv4_2, 'conv4_3')\n",
    "        self.pool4 = self.pooling_layer(self.conv4_3, 'pool4')\n",
    "        \n",
    "        self.conv5_1 = self.conv_layer(self.pool4, 'conv5_1')\n",
    "        self.conv5_2 = self.conv_layer(self.conv5_1, 'conv5_2')\n",
    "        self.conv5_3 = self.conv_layer(self.conv5_2, 'conv5_3')\n",
    "        self.pool5 = self.pooling_layer(self.conv5_3, 'pool5')\n",
    "        \n",
    "        '''\n",
    "        #展开 -> 全连接\n",
    "        self.flatten5 = self.flatten_layer(self.pool5, 'flatten')\n",
    "        self.fc6 = self.fc_layer(self.flatten5, 'fc6')\n",
    "        self.fc7 = self.fc_layer(self.fc6, 'fc7')\n",
    "        \n",
    "        #最后的fc8输出1k个值，给softmax()去计算概率分布\n",
    "        #所以fc8不需要activation\n",
    "        self.fc8 = self.fc_layer(self.fc7, 'fc8', activation = None)\n",
    "        \n",
    "        #计算softmax\n",
    "        self.prob = tf.nn.softmax(self.fc8, name = 'prob')\n",
    "        '''\n",
    "        \n",
    "        print(\"Building model finished: %4ds\" % (time.time() - start_time))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试我们的VGGNet 类是否成功："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Building model finished:    0s\n"
     ]
    }
   ],
   "source": [
    "#加载vgg16\n",
    "vgg16_npy_path = '../../../other_datasets/vgg16.npy'\n",
    "data_dict = np.load(vgg16_npy_path, encoding = 'latin1').item() #加item()是为了创建成字典\n",
    "\n",
    "vgg16_for_result = VGGNet(data_dict)\n",
    "content = tf.placeholder(tf.float32, shape = [1,224,224,3])\n",
    "vgg16_for_result.build(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始风格转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_img_path = './others/resized_citi.JPG'\n",
    "style_img_path = './others/resized_starry_night.jpg'\n",
    "\n",
    "#训练次数\n",
    "num_steps = 100\n",
    "\n",
    "#学习率\n",
    "learning_rate = 10\n",
    "\n",
    "#风格系数，内容系数\n",
    "lambda_c = 0 #内容\n",
    "lambda_s = 1 #风格\n",
    "\n",
    "#输出文件夹, 每一步都有新图像，将所有图像输入这个文件夹\n",
    "output_dir = './005_style_rebuild_output_shallow'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Building model finished:    0s\n",
      "Building model...\n",
      "Building model finished:    0s\n",
      "Building model...\n",
      "Building model finished:    0s\n"
     ]
    }
   ],
   "source": [
    "#定义随机的初始图片：\n",
    "def initial_result(shape, mean, stddev):\n",
    "    #用正态分布初始化\n",
    "    initial = tf.truncated_normal(shape, mean = mean, stddev = stddev) #截断的正态分布\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#将风格，内容图像读取进来：\n",
    "def read_img(img_name):\n",
    "    #PIL里面的函数,读取img\n",
    "    img = Image.open(img_name) \n",
    "    \n",
    "    #变成numpy的矩阵\n",
    "    np_img = np.array(img) #（224,224,3)\n",
    "    \n",
    "    #矩阵是 (224, 224, 3), 需要改变成4维：\n",
    "    #因为只有一个样本，所以可以不用reshape，而是直接用[np_img]，将np_img包含在一个list [] 里面\n",
    "    #如此就直接多了一个维度，变成(1, 224, 224, 3)\n",
    "    np_img = np.asarray([np_img], dtype = np.int32)\n",
    "    \n",
    "    return np_img\n",
    "\n",
    "#计算Gram矩阵，用于风格损失：\n",
    "def gram_matrix(x):\n",
    "    \"\"\"Calculates gram matrix\n",
    "    Args:\n",
    "    - x: features extracted from VGG Net. shape: [1, width, height, channel]\n",
    "    \"\"\"\n",
    "    batch, w, h, ch = x.get_shape().as_list()\n",
    "    features = tf.reshape(x, [batch, h*w, ch]) #将w，h合成一个维度\n",
    "    \n",
    "    #针对channel，两两进行计算余弦相似度，得到gram matrix\n",
    "    #1. 我们的features的第一个维度batch，永远都等于1\n",
    "    #2. 我们的features的后两个维度，我们可以看成一个二维矩阵，假设叫A：行是h*w, 列是ch\n",
    "    #3. 计算所有channel两两之间的相似度，我们可以看做将A矩阵中，抽取两个列进行点乘\n",
    "    #4. 假设有k个channel，我们最后的gram matrix是k * k 矩阵，其中第i行，第j列代表第i个ch和第j个ch的相似度\n",
    "    #5. 计算ch * ch的点乘，可以用 (ch * hw) * (hw * ch)进行点乘，也就是features的转置，点乘features\n",
    "    #6. 矩阵乘法用tf.matmul(), 其中adjoin_a = True表示第一个参数要转置\n",
    "    #7. 担心点乘后值过大，所以我们还会除以一个常数：也就是各个维度的乘积，例如224*224*3\n",
    "    gram = tf.matmul(features, features, adjoint_a = True) \\\n",
    "            / tf.constant(ch * w * h, tf.float32)\n",
    "    return gram\n",
    "\n",
    "#初始化图像: mean是[0,255]的中间值\n",
    "result = initial_result((1, 224, 224, 3), 127.5, 20)\n",
    "\n",
    "#读取风格，内容图片\n",
    "content_val = read_img(content_img_path)\n",
    "style_val = read_img(style_img_path)\n",
    "\n",
    "#用feed_dict塞入，所以我们需要先创建placeholder\n",
    "content = tf.placeholder(tf.float32, shape = [1,224,224,3])\n",
    "style = tf.placeholder(tf.float32, shape = [1,224,224,3])\n",
    "\n",
    "#将三张图输入vggnet，然后提取特征\n",
    "data_dict = np.load(vgg16_npy_path, encoding = 'latin1').item()\n",
    "\n",
    "#创建三个vggnet，都是同样的参数\n",
    "#给内容图像创建vggnet\n",
    "vgg_for_content = VGGNet(data_dict)\n",
    "\n",
    "#给风格图像创建vggnet\n",
    "vgg_for_style = VGGNet(data_dict)\n",
    "\n",
    "#给结果图像创建vggnet\n",
    "vgg_for_result = VGGNet(data_dict)\n",
    "\n",
    "#调用build函数，完成vggnet构建\n",
    "#content和vgg_for_content进行关联\n",
    "vgg_for_content.build(content)\n",
    "vgg_for_style.build(style)\n",
    "vgg_for_result.build(result)\n",
    "\n",
    "#vggnet的每个层都可以进行特征提取\n",
    "#content：越底层提取的特征越清晰\n",
    "#以下是提取了5个特征，我们先注释掉后3个\n",
    "content_features = [\n",
    "    vgg_for_content.conv1_2,\n",
    "    vgg_for_content.conv2_2,\n",
    "    #vgg_for_content.conv3_3,\n",
    "    #vgg_for_content.conv4_3,\n",
    "    #vgg_for_content.conv5_3,\n",
    "]\n",
    "\n",
    "#给内容图像提取了哪些特征，就要对结果图像提取哪些特征\n",
    "result_content_features = [\n",
    "    vgg_for_result.conv1_2,\n",
    "    vgg_for_result.conv2_2,\n",
    "    #vgg_for_result.conv3_3,\n",
    "    #vgg_for_result.conv4_3,\n",
    "    #vgg_for_result.conv5_3,\n",
    "]\n",
    "\n",
    "#style: 越高层越好，所以注释掉底层\n",
    "style_features = [\n",
    "    vgg_for_style.conv1_2,\n",
    "    vgg_for_style.conv2_2,\n",
    "    #vgg_for_style.conv3_3,\n",
    "    #vgg_for_style.conv4_3,\n",
    "    #vgg_for_style.conv5_3,\n",
    "]\n",
    "\n",
    "#给风格图像的风格特征计算gram矩阵：\n",
    "style_gram = [gram_matrix(feature) for feature in style_features]\n",
    "\n",
    "#给风格图像提取了哪些特征，就要对结果图像提取哪些特征\n",
    "result_style_features = [\n",
    "    vgg_for_result.conv1_2,\n",
    "    vgg_for_result.conv2_2,\n",
    "    #vgg_for_result.conv3_3,\n",
    "    #vgg_for_result.conv4_3,\n",
    "    #vgg_for_result.conv5_3,\n",
    "]\n",
    "\n",
    "#给结果图像的风格特征计算gram矩阵：\n",
    "result_style_gram = [gram_matrix(feature) for feature in result_style_features]\n",
    "\n",
    "#计算内容损失 + 风格损失\n",
    "#1. 内容损失：是每一层损失的加和\n",
    "#zip: 将两个数组绑定在一起\n",
    "#例如：[1,2], [3,4] \n",
    "#zip([1,2], [3,4]) -> [(1,3), (2,4)]\n",
    "#两个list -> 一个list中两个pair：(1,3) 和 (2,4)\n",
    "#想象：将两个list竖着放，然后横着拿出来\n",
    "content_loss = tf.zeros(1, tf.float32) #一个数，是标量\n",
    "for c, c_ in zip(content_features, result_content_features):\n",
    "    #c, c_ = (content_features[0], result_content_features[0])\n",
    "    #c, c_ = (content_features[1], result_content_features[1])\n",
    "    #内容损失是提取的特征：c, c_的平方差，再求平均。\n",
    "    #平均是长宽高所有维度的平均，因为shape = [1, width, height, channel], 所以axis = [1,2,3]\n",
    "    content_loss += tf.reduce_mean((c - c_) ** 2, [1,2,3])\n",
    "    \n",
    "#2. 风格损失\n",
    "#将某一层的特征提取出来，会得到feature_map\n",
    "style_loss = tf.zeros(1, tf.float32)\n",
    "for s, s_ in zip(style_gram, result_style_gram):\n",
    "    #平方差损失函数，在1，2维度求均值。因为gram_matrix()已经将width和height降成1维。所以现在第零维是batch，第一维是w*h, 第二维度是ch\n",
    "    style_loss += tf.reduce_mean((s - s_) ** 2, [1,2])\n",
    "\n",
    "#loss加权\n",
    "loss = content_loss * lambda_c + style_loss * lambda_s\n",
    "    \n",
    "#给损失函数计算梯度\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 图像风格转换的训练流程图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1, loss_value: 1812998.5000, content_loss: 349758.5625, style_loss: 1812998.5000\n",
      "step: 2, loss_value: 984988.2500, content_loss: 353371.4375, style_loss: 984988.2500\n",
      "step: 3, loss_value: 504617.1250, content_loss: 414270.0312, style_loss: 504617.1250\n",
      "step: 4, loss_value: 732134.5625, content_loss: 489737.4062, style_loss: 732134.5625\n",
      "step: 5, loss_value: 485500.2812, content_loss: 498055.9375, style_loss: 485500.2812\n",
      "step: 6, loss_value: 206284.4375, content_loss: 484390.9688, style_loss: 206284.4375\n",
      "step: 7, loss_value: 133500.6719, content_loss: 473564.3125, style_loss: 133500.6719\n",
      "step: 8, loss_value: 145045.9062, content_loss: 473493.8750, style_loss: 145045.9062\n",
      "step: 9, loss_value: 134386.9062, content_loss: 483613.7500, style_loss: 134386.9062\n",
      "step: 10, loss_value: 111831.5781, content_loss: 500374.2500, style_loss: 111831.5781\n",
      "step: 11, loss_value: 122282.8750, content_loss: 516011.0625, style_loss: 122282.8750\n",
      "step: 12, loss_value: 142127.5156, content_loss: 518321.0938, style_loss: 142127.5156\n",
      "step: 13, loss_value: 124293.9766, content_loss: 505056.0312, style_loss: 124293.9766\n",
      "step: 14, loss_value: 95494.8750, content_loss: 484357.5938, style_loss: 95494.8750\n",
      "step: 15, loss_value: 88997.7344, content_loss: 465150.7812, style_loss: 88997.7344\n",
      "step: 16, loss_value: 94909.1406, content_loss: 452688.4375, style_loss: 94909.1406\n",
      "step: 17, loss_value: 91442.4062, content_loss: 448343.0000, style_loss: 91442.4062\n",
      "step: 18, loss_value: 77519.6719, content_loss: 451467.3438, style_loss: 77519.6719\n",
      "step: 19, loss_value: 68521.1172, content_loss: 460049.6250, style_loss: 68521.1172\n",
      "step: 20, loss_value: 70318.6641, content_loss: 470103.0938, style_loss: 70318.6641\n",
      "step: 21, loss_value: 68867.1016, content_loss: 476806.7500, style_loss: 68867.1016\n",
      "step: 22, loss_value: 59971.7266, content_loss: 478483.6875, style_loss: 59971.7266\n",
      "step: 23, loss_value: 55436.1680, content_loss: 477255.5000, style_loss: 55436.1680\n",
      "step: 24, loss_value: 56790.3047, content_loss: 476053.0312, style_loss: 56790.3047\n",
      "step: 25, loss_value: 55357.7695, content_loss: 476630.2188, style_loss: 55357.7695\n",
      "step: 26, loss_value: 49477.3125, content_loss: 479110.9375, style_loss: 49477.3125\n",
      "step: 27, loss_value: 45663.0859, content_loss: 482181.6562, style_loss: 45663.0859\n",
      "step: 28, loss_value: 46095.8281, content_loss: 483615.0625, style_loss: 46095.8281\n",
      "step: 29, loss_value: 45004.3047, content_loss: 481748.5625, style_loss: 45004.3047\n",
      "step: 30, loss_value: 41042.5625, content_loss: 477137.2500, style_loss: 41042.5625\n",
      "step: 31, loss_value: 39058.5625, content_loss: 472211.3125, style_loss: 39058.5625\n",
      "step: 32, loss_value: 39387.0781, content_loss: 469410.2188, style_loss: 39387.0781\n",
      "step: 33, loss_value: 38317.8906, content_loss: 469892.8438, style_loss: 38317.8906\n",
      "step: 34, loss_value: 35622.4336, content_loss: 473261.7812, style_loss: 35622.4336\n",
      "step: 35, loss_value: 34249.1953, content_loss: 477830.3438, style_loss: 34249.1953\n",
      "step: 36, loss_value: 34265.7969, content_loss: 481275.9375, style_loss: 34265.7969\n",
      "step: 37, loss_value: 33048.9219, content_loss: 481972.3438, style_loss: 33048.9219\n",
      "step: 38, loss_value: 31006.9297, content_loss: 480123.7188, style_loss: 31006.9297\n",
      "step: 39, loss_value: 30162.2520, content_loss: 477367.8125, style_loss: 30162.2520\n",
      "step: 40, loss_value: 29751.5703, content_loss: 475421.6562, style_loss: 29751.5703\n",
      "step: 41, loss_value: 28371.9766, content_loss: 475118.3438, style_loss: 28371.9766\n",
      "step: 42, loss_value: 26987.3145, content_loss: 476190.3438, style_loss: 26987.3145\n",
      "step: 43, loss_value: 26630.6953, content_loss: 477540.0000, style_loss: 26630.6953\n",
      "step: 44, loss_value: 26145.8223, content_loss: 478016.4375, style_loss: 26145.8223\n",
      "step: 45, loss_value: 24934.5625, content_loss: 477385.0625, style_loss: 24934.5625\n",
      "step: 46, loss_value: 24114.0742, content_loss: 476490.9688, style_loss: 24114.0742\n",
      "step: 47, loss_value: 23688.6289, content_loss: 476471.0000, style_loss: 23688.6289\n",
      "step: 48, loss_value: 22762.6523, content_loss: 477835.2500, style_loss: 22762.6523\n",
      "step: 49, loss_value: 21744.6719, content_loss: 480137.9375, style_loss: 21744.6719\n",
      "step: 50, loss_value: 21351.1758, content_loss: 482233.8750, style_loss: 21351.1758\n",
      "step: 51, loss_value: 21001.5840, content_loss: 483040.3125, style_loss: 21001.5840\n",
      "step: 52, loss_value: 20278.6836, content_loss: 482350.3750, style_loss: 20278.6836\n",
      "step: 53, loss_value: 19721.3477, content_loss: 480972.5000, style_loss: 19721.3477\n",
      "step: 54, loss_value: 19304.8535, content_loss: 480008.3750, style_loss: 19304.8535\n",
      "step: 55, loss_value: 18595.3965, content_loss: 480047.7812, style_loss: 18595.3965\n",
      "step: 56, loss_value: 17906.2910, content_loss: 480868.0938, style_loss: 17906.2910\n",
      "step: 57, loss_value: 17544.1777, content_loss: 481682.3438, style_loss: 17544.1777\n",
      "step: 58, loss_value: 17111.1738, content_loss: 481798.2188, style_loss: 17111.1738\n",
      "step: 59, loss_value: 16533.6523, content_loss: 481239.6250, style_loss: 16533.6523\n",
      "step: 60, loss_value: 16135.1855, content_loss: 480688.6250, style_loss: 16135.1855\n",
      "step: 61, loss_value: 15765.8105, content_loss: 480845.1562, style_loss: 15765.8105\n",
      "step: 62, loss_value: 15270.0771, content_loss: 481843.9062, style_loss: 15270.0771\n",
      "step: 63, loss_value: 14882.8340, content_loss: 483174.1875, style_loss: 14882.8340\n",
      "step: 64, loss_value: 14576.0020, content_loss: 484074.8438, style_loss: 14576.0020\n",
      "step: 65, loss_value: 14138.5879, content_loss: 484160.7812, style_loss: 14138.5879\n",
      "step: 66, loss_value: 13713.3340, content_loss: 483704.2188, style_loss: 13713.3340\n",
      "step: 67, loss_value: 13381.5566, content_loss: 483322.0000, style_loss: 13381.5566\n",
      "step: 68, loss_value: 13004.0889, content_loss: 483412.3750, style_loss: 13004.0889\n",
      "step: 69, loss_value: 12645.1221, content_loss: 483862.6250, style_loss: 12645.1221\n",
      "step: 70, loss_value: 12378.7598, content_loss: 484209.0312, style_loss: 12378.7598\n",
      "step: 71, loss_value: 12070.8457, content_loss: 484095.4688, style_loss: 12070.8457\n",
      "step: 72, loss_value: 11727.9844, content_loss: 483637.1250, style_loss: 11727.9844\n",
      "step: 73, loss_value: 11441.3008, content_loss: 483308.3125, style_loss: 11441.3008\n",
      "step: 74, loss_value: 11142.9365, content_loss: 483486.7500, style_loss: 11142.9365\n",
      "step: 75, loss_value: 10830.2617, content_loss: 484123.1562, style_loss: 10830.2617\n",
      "step: 76, loss_value: 10570.5645, content_loss: 484797.0625, style_loss: 10570.5645\n",
      "step: 77, loss_value: 10307.3008, content_loss: 485095.3438, style_loss: 10307.3008\n",
      "step: 78, loss_value: 10025.8945, content_loss: 484972.5625, style_loss: 10025.8945\n",
      "step: 79, loss_value: 9782.3945, content_loss: 484744.0938, style_loss: 9782.3945\n",
      "step: 80, loss_value: 9541.6006, content_loss: 484739.7812, style_loss: 9541.6006\n",
      "step: 81, loss_value: 9291.1035, content_loss: 485001.5625, style_loss: 9291.1035\n",
      "step: 82, loss_value: 9069.2002, content_loss: 485280.9375, style_loss: 9069.2002\n",
      "step: 83, loss_value: 8846.2422, content_loss: 485321.3125, style_loss: 8846.2422\n",
      "step: 84, loss_value: 8612.4785, content_loss: 485131.3125, style_loss: 8612.4785\n",
      "step: 85, loss_value: 8400.3311, content_loss: 484971.3750, style_loss: 8400.3311\n",
      "step: 86, loss_value: 8191.4756, content_loss: 485078.6875, style_loss: 8191.4756\n",
      "step: 87, loss_value: 7982.5894, content_loss: 485430.2188, style_loss: 7982.5894\n",
      "step: 88, loss_value: 7793.5547, content_loss: 485767.8750, style_loss: 7793.5547\n",
      "step: 89, loss_value: 7603.6914, content_loss: 485854.1875, style_loss: 7603.6914\n",
      "step: 90, loss_value: 7412.3018, content_loss: 485695.6562, style_loss: 7412.3018\n",
      "step: 91, loss_value: 7235.0981, content_loss: 485509.8750, style_loss: 7235.0981\n",
      "step: 92, loss_value: 7058.0244, content_loss: 485489.0938, style_loss: 7058.0244\n",
      "step: 93, loss_value: 6884.6934, content_loss: 485623.2188, style_loss: 6884.6934\n",
      "step: 94, loss_value: 6723.2227, content_loss: 485748.1250, style_loss: 6723.2227\n",
      "step: 95, loss_value: 6560.4932, content_loss: 485751.8750, style_loss: 6560.4932\n",
      "step: 96, loss_value: 6402.3403, content_loss: 485702.4375, style_loss: 6402.3403\n",
      "step: 97, loss_value: 6252.9727, content_loss: 485760.2188, style_loss: 6252.9727\n",
      "step: 98, loss_value: 6104.1094, content_loss: 485984.0625, style_loss: 6104.1094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 99, loss_value: 5962.0591, content_loss: 486252.9062, style_loss: 5962.0591\n",
      "step: 100, loss_value: 5825.5918, content_loss: 486383.5938, style_loss: 5825.5918\n"
     ]
    }
   ],
   "source": [
    "#定义初始化op\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op) #初始化变量，在图像风格转换中，变量只有一个：结果图像（对结果图像进行梯度下降）\n",
    "    \n",
    "    #开始训练：\n",
    "    for step in range(num_steps):\n",
    "        #最后一个_是因为要训练\n",
    "        loss_value, content_loss_value, style_loss_value, _ \\\n",
    "            = sess.run([loss, content_loss, style_loss, train_op],\n",
    "                        feed_dict = {\n",
    "                            content: content_val,\n",
    "                            style: style_val,\n",
    "                        })\n",
    "        print('step: %d, loss_value: %8.4f, content_loss: %8.4f, style_loss: %8.4f' \\\n",
    "             % (step+1, \n",
    "                loss_value[0],  #之所以是[0], 是因为我们是四维的，第一个维度只有一个元素，所以取[0]\n",
    "                content_loss_value[0], \n",
    "                style_loss_value[0]))\n",
    "        \n",
    "        #获得每一步的结果图像，输出到dir中\n",
    "        result_img_path = os.path.join(\n",
    "            output_dir, 'result-%05d.jpg' % (step + 1))\n",
    "        \n",
    "        #将参数值，也就是结果图像的变量值，取出来\n",
    "        #result是之前定义的随机噪声 result = initial_result((1, 224, 224, 3), 127.5, 20)\n",
    "        result_val = result.eval(sess)[0] #因为第一个维度是只有一个元素，所以取出用[0]\n",
    "        result_val = np.clip(result_val, 0, 255) #clip的作用：裁剪，将小于0的设成0，大于255的设置成255\n",
    "\n",
    "        #之前是float，现在转换成int\n",
    "        img_arr = np.asarray(result_val, np.uint8) \n",
    "        \n",
    "        #用PIL转换成图片\n",
    "        img = Image.fromarray(img_arr) #可以将numpy数组转换成图片\n",
    "        img.save(result_img_path)\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为何之前设置这样的风格系数，内容系数\n",
    "1. 首先我们的style loss的计算中，gram matrix需要除以一个比较大的数tf.constant(ch * w * h, tf.float32)，导致了style loss比较小\n",
    "2. gram = tf.matmul(features, features, adjoint_a = True) \\\n",
    "            / tf.constant(ch * w * h, tf.float32)\n",
    "3. 既然style loss小，content loss大，为了均衡两者，所以一个乘500，一个乘0.1\n",
    "4. lambda_c = 0.1 #内容， lambda_s = 500 #风格\n",
    "5. style loss: 1.7 * 500 = 850\n",
    "6. content loss: 12997 * 0.1 = 1299"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 这个文件的泛化能力\n",
    "1. 首先可以通过注释和取消注释来选取不同层的特征：\n",
    "\n",
    "style_features = [\n",
    "    #vgg_for_style.conv1_2,\n",
    "    #vgg_for_style.conv2_2,\n",
    "    #vgg_for_style.conv3_3,\n",
    "    vgg_for_style.conv4_3,\n",
    "    #vgg_for_style.conv5_3,\n",
    "]\n",
    "\n",
    "2. 可以只要重建图片，或者只重建风格：\n",
    "\n",
    "lambda_c = 0 #只重建图片\n",
    "\n",
    "lambda_s = 5000 #只重建风格\n",
    "\n",
    "3. 可以为style loss 和 content loss的不同层添加系数：\n",
    "\n",
    "for c, c_ in zip(content_features, result_content_features):\n",
    "\n",
    "    content_loss += tf.reduce_mean((c - c_) ** 2, [1,2,3])\n",
    "    \n",
    "for s, s_ in zip(style_gram, result_style_gram):\n",
    "\n",
    "    style_loss += tf.reduce_mean((s - s_) ** 2, [1,2])\n",
    "    \n",
    "例如第一层的c和c_可以乘系数0.1\n",
    "\n",
    "content_loss += 0.1 * tf.reduce_mean((c - c_) ** 2, [1,2,3])\n",
    "\n",
    "例如第二层的c和c_可以乘系数0.5\n",
    "\n",
    "content_loss += 0.5 * tf.reduce_mean((c - c_) ** 2, [1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
