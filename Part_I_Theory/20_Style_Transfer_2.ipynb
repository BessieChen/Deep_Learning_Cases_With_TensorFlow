{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建计算图\n",
    "修改：我们做风格转换不需要最后的fc，并且fc含有参数多，费时，所以我们这里注释掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image #图像处理库\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/bessie/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG中自带常量，VGG有将图片进行预处理，其中一个步骤是normalization:\n",
    "#减去image_net的RGB通道的各个均值\n",
    "VGG_MEAN = [103.939, 116.779, 123.68] #在vggnet的code中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNet:\n",
    "    \"\"\"Builds VGG-16 net structure,\n",
    "        load parameters from pre-trained models.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dict):\n",
    "        self.data_dict = data_dict\n",
    "        \n",
    "    def get_conv_filter(self, name): #抽取卷积参数\n",
    "        \"\"\"eg. conv1_1 = data_dict['conv1_1']\"\"\"\n",
    "        #tf.constant() #因为模型是预处理好的，所以我们不会改变参数，所以定义为常量。\n",
    "        #另一个方法：可以设置成trainable = False\n",
    "        return tf.constant(self.data_dict[name][0], name = 'conv') #这里应该是w,b中的w\n",
    "    \n",
    "    def get_fc_weight(self, name):\n",
    "        return tf.constant(self.data_dict[name][0], name = 'fc') #这里应该是w,b中的w\n",
    "    \n",
    "    def get_bias(self, name):\n",
    "        return tf.constant(self.data_dict[name][1], name = 'bias') #这里应该是w,b中的b\n",
    "    \n",
    "    #创建卷积层，池化层，全连接层\n",
    "    def conv_layer(self, x, name):\n",
    "        \"\"\"Builds convolution layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            #加上name_scope是命名规范：\n",
    "            #1. 防止命名冲突\n",
    "            #2. tensorboard打印名字更加清晰规范\n",
    "            conv_w = self.get_conv_filter(name)\n",
    "            conv_b = self.get_bias(name)\n",
    "            \n",
    "            #现在不再使用tf.layers.conv2d(),因为我们已经有了pre-trained的参数\n",
    "            #现在使用更基础的api: tf.nn.conv2d()\n",
    "            h = tf.nn.conv2d(x, conv_w, [1,1,1,1], padding = 'SAME') #x是input，[1,1,1,1]是strides步长，因为这里x是四维，所以我们输入四个数\n",
    "            h = tf.nn.bias_add(h, conv_b)\n",
    "            \n",
    "            #激活函数\n",
    "            h = tf.nn.relu(h)\n",
    "            return h\n",
    "        \n",
    "    #也是使用tf.nn.max_pool()而不是tf.layers.max_pooling2d\n",
    "    def pooling_layer(self, x, name):\n",
    "        \"\"\"Builds pooling layer.\"\"\"\n",
    "        return tf.nn.max_pool(x,\n",
    "                                 ksize = [1,2,2,1], #因为是按照长和宽来池化，所以是中间两个维度是2，其余维度是1\n",
    "                                 strides = [1,2,2,1], \n",
    "                                 padding = 'SAME', \n",
    "                                 name = name) \n",
    "    \n",
    "    \n",
    "    def fc_layer(self, x, name, activation = tf.nn.relu):\n",
    "        \"\"\"Builds fully-connected layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            fc_w = self.get_fc_weight(name)\n",
    "            fc_b = self.get_bias(name)\n",
    "            h = tf.matmul(x, fc_w) #让输入x与w进行操作\n",
    "            h = tf.nn.bias_add(h, fc_b)\n",
    "            if activation == None:\n",
    "                return h\n",
    "            else:\n",
    "                return activation(h)\n",
    "            \n",
    "    \n",
    "    #创建展平功能，展平后输入给全连接层：做的是reshape操作，我们需要知道reshape之后的size有多大\n",
    "    #展平之后，需要的长宽厚的乘积\n",
    "    def flatten_layer(self, x, name):\n",
    "        \"\"\"Builds flatten layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            #[batch_size, img_width, img_height, channel]\n",
    "            x_shape = x.get_shape().as_list()\n",
    "            dim = 1\n",
    "            for d in x_shape[1:]:\n",
    "                dim *= d\n",
    "            x = tf.reshape(x, [-1, dim]) #这里的-1，是reshape推断出来的，也是我们的batch_size, 你也可以写成[x_shape[0], dim]\n",
    "            return x\n",
    "       \n",
    "    #建立vgg\n",
    "    #我们现在就要做图像的风格转换，需要的图片只有一个，所以第一个维度是1\n",
    "    #vggnet的设置中，图像大小是224*224\n",
    "    def build(self, x_rgb):\n",
    "        \"\"\"BUild VGG16 network structure.\n",
    "        Args:\n",
    "        - x_rgb: eg. [1, 224, 224, 3]\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        print(\"Building model...\")\n",
    "        \n",
    "        #每个通道减去均值VGG_MEAN，先拆分通道\n",
    "        #复习：tf.split() 之前用于：深度可分离卷积，数据增强\n",
    "        r, g, b = tf.split(x_rgb, [1,1,1], axis = 3) #切分成三通道：[1,1,1]\n",
    "        \n",
    "        #去除均值后，需要合并。这里注意vggnet输入的通道顺序是BGR\n",
    "        #意味着之前写的VGG_MEAN的三个数分别是 BGR 的均值\n",
    "        x_bgr = tf.concat([b - VGG_MEAN[0], \n",
    "                           g - VGG_MEAN[1],\n",
    "                           r - VGG_MEAN[2]],\n",
    "                          axis = 3) #在第四个维度，channel上合并\n",
    "        \n",
    "        #预处理之后，判断一下我们的维度是 224*224*3\n",
    "        assert x_bgr.get_shape().as_list()[1:] == [224,224,3]\n",
    "        \n",
    "        #构建前两个卷积层：\n",
    "        #vgg16：\n",
    "        #第一个结构(stage)：两个卷积层 -> 一个池化层\n",
    "        #第二个结构：两个卷积层 -> 一个池化层\n",
    "        #第3个结构：3个卷积层 -> 一个池化层\n",
    "        #第4个结构：3个卷积层 -> 一个池化层\n",
    "        #第5个结构：3个卷积层 -> 一个池化层\n",
    "        #第6个结构：3个全连接层\n",
    "        #2*2 + 3*3 + 3 = 4 + 9 + 3 = 16, 也就是vgg16 \n",
    "        \n",
    "        ##注意：self.conv_layer(xx,yy)第二个参数的名字必须是data_dict.keys()中的\n",
    "        #dict_keys(['conv5_1', 'fc6', 'conv5_3', 'conv5_2', 'fc8', 'fc7', 'conv4_1', 'conv4_2', 'conv4_3', 'conv3_3', 'conv3_2', 'conv3_1', 'conv1_1', 'conv1_2', 'conv2_2', 'conv2_1'])\n",
    "        #我们将每一个层，设置成了成员变量, eg. self.conv1_1, 可能会用其中的某一层计算风格损失或者内容损失，设置成成员变量我们以后可以方便使用\n",
    "        self.conv1_1 = self.conv_layer(x_bgr, 'conv1_1')\n",
    "        self.conv1_2 = self.conv_layer(self.conv1_1, 'conv1_2')\n",
    "        self.pool1 = self.pooling_layer(self.conv1_2, 'pool1') #pool1因为不是data_dict里面预处理好的，所以我们可以随意命名, 例如pool1。并且可以不用将它设置成成员函数，只不过这里为了统一起见\n",
    "        \n",
    "        self.conv2_1 = self.conv_layer(self.pool1, 'conv2_1')\n",
    "        self.conv2_2 = self.conv_layer(self.conv2_1, 'conv2_2')\n",
    "        self.pool2 = self.pooling_layer(self.conv2_2, 'pool2')\n",
    "        \n",
    "        self.conv3_1 = self.conv_layer(self.pool2, 'conv3_1')\n",
    "        self.conv3_2 = self.conv_layer(self.conv3_1, 'conv3_2')\n",
    "        self.conv3_3 = self.conv_layer(self.conv3_2, 'conv3_3')\n",
    "        self.pool3 = self.pooling_layer(self.conv3_3, 'pool3')\n",
    "        \n",
    "        self.conv4_1 = self.conv_layer(self.pool3, 'conv4_1')\n",
    "        self.conv4_2 = self.conv_layer(self.conv4_1, 'conv4_2')\n",
    "        self.conv4_3 = self.conv_layer(self.conv4_2, 'conv4_3')\n",
    "        self.pool4 = self.pooling_layer(self.conv4_3, 'pool4')\n",
    "        \n",
    "        self.conv5_1 = self.conv_layer(self.pool4, 'conv5_1')\n",
    "        self.conv5_2 = self.conv_layer(self.conv5_1, 'conv5_2')\n",
    "        self.conv5_3 = self.conv_layer(self.conv5_2, 'conv5_3')\n",
    "        self.pool5 = self.pooling_layer(self.conv5_3, 'pool5')\n",
    "        \n",
    "        '''\n",
    "        #展开 -> 全连接\n",
    "        self.flatten5 = self.flatten_layer(self.pool5, 'flatten')\n",
    "        self.fc6 = self.fc_layer(self.flatten5, 'fc6')\n",
    "        self.fc7 = self.fc_layer(self.fc6, 'fc7')\n",
    "        \n",
    "        #最后的fc8输出1k个值，给softmax()去计算概率分布\n",
    "        #所以fc8不需要activation\n",
    "        self.fc8 = self.fc_layer(self.fc7, 'fc8', activation = None)\n",
    "        \n",
    "        #计算softmax\n",
    "        self.prob = tf.nn.softmax(self.fc8, name = 'prob')\n",
    "        '''\n",
    "        \n",
    "        print(\"Building model finished: %4ds\" % (time.time() - start_time))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试我们的VGGNet 类是否成功："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Building model finished:    0s\n"
     ]
    }
   ],
   "source": [
    "#加载vgg16\n",
    "vgg16_npy_path = '../../../other_datasets/vgg16.npy'\n",
    "data_dict = np.load(vgg16_npy_path, encoding = 'latin1').item() #加item()是为了创建成字典\n",
    "\n",
    "vgg16_for_result = VGGNet(data_dict)\n",
    "content = tf.placeholder(tf.float32, shape = [1,224,224,3])\n",
    "vgg16_for_result.build(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将图片都设置为大小是 224 * 224 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 224, 224, 3)\n",
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "content_img_path = './others/citi.JPG'\n",
    "\n",
    "img_string = tf.read_file(content_img_path)\n",
    "img_decoded = tf.image.decode_image(img_string)\n",
    "\n",
    "sess = tf.Session()\n",
    "#用sess执行这个图\n",
    "img_decoded_val = sess.run(img_decoded)\n",
    "\n",
    "#因为tf.image.resize_bicubic()处理的是batch数据，所以需要4维输入，第一维就是图片数\n",
    "shape = img_decoded_val.shape\n",
    "img_decoded = tf.reshape(img_decoded, [1, shape[0], shape[1], shape[2]])\n",
    "\n",
    "#在img_decoded上面加工：\n",
    "resize_img = tf.image.resize_bicubic(img_decoded, [224, 224])\n",
    "\n",
    "sess = tf.Session()\n",
    "img_decoded_val = sess.run(resize_img)\n",
    "\n",
    "#需要将4维改成3维：\n",
    "print(img_decoded_val.shape)#(1, 224, 224, 3)\n",
    "img_decoded_val = img_decoded_val.reshape((224, 224, 3))\n",
    "print(img_decoded_val.shape)\n",
    "\n",
    "#需要做类型变换：从[0,1]之间的小数float 改为 [0,255]integer \n",
    "img_decoded_val = np.asarray(img_decoded_val, np.uint8)\n",
    "\n",
    "resize_img_path = os.path.join('./others', 'resized_citi.jpg')\n",
    "\n",
    "img = Image.fromarray(img_decoded_val) #可以将numpy数组转换成图片\n",
    "img.save(resize_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 224, 224, 3)\n",
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "style_img_path = './others/starry_night.jpeg'\n",
    "\n",
    "img_string = tf.read_file(style_img_path)\n",
    "img_decoded = tf.image.decode_image(img_string)\n",
    "\n",
    "sess = tf.Session()\n",
    "#用sess执行这个图\n",
    "img_decoded_val = sess.run(img_decoded)\n",
    "\n",
    "#因为tf.image.resize_bicubic()处理的是batch数据，所以需要4维输入，第一维就是图片数\n",
    "shape = img_decoded_val.shape\n",
    "img_decoded = tf.reshape(img_decoded, [1, shape[0], shape[1], shape[2]])\n",
    "\n",
    "#在img_decoded上面加工：\n",
    "resize_img = tf.image.resize_bicubic(img_decoded, [224, 224])\n",
    "\n",
    "sess = tf.Session()\n",
    "img_decoded_val = sess.run(resize_img)\n",
    "\n",
    "#需要将4维改成3维：\n",
    "print(img_decoded_val.shape)##(1, 224, 224, 3)\n",
    "img_decoded_val = img_decoded_val.reshape((224, 224, 3))\n",
    "print(img_decoded_val.shape)\n",
    "\n",
    "#需要做类型变换：从[0,1]之间的小数float 改为 [0,255]integer \n",
    "img_decoded_val = np.asarray(img_decoded_val, np.uint8)\n",
    "\n",
    "resize_img_path = os.path.join('./others', 'resized_starry_night.jpg')\n",
    "\n",
    "img = Image.fromarray(img_decoded_val) #可以将numpy数组转换成图片\n",
    "img.save(resize_img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始风格转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_img_path = './others/resized_citi.JPG'\n",
    "style_img_path = './others/resized_starry_night.jpg'\n",
    "\n",
    "#训练次数\n",
    "num_steps = 100\n",
    "\n",
    "#学习率\n",
    "learning_rate = 10\n",
    "\n",
    "#风格系数，内容系数\n",
    "lambda_c = 0.1 #内容\n",
    "lambda_s = 5000 #风格\n",
    "\n",
    "#输出文件夹, 每一步都有新图像，将所有图像输入这个文件夹\n",
    "output_dir = './style_transfer_output'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Building model finished:    0s\n",
      "Building model...\n",
      "Building model finished:    0s\n",
      "Building model...\n",
      "Building model finished:    0s\n"
     ]
    }
   ],
   "source": [
    "#定义随机的初始图片：\n",
    "def initial_result(shape, mean, stddev):\n",
    "    #用正态分布初始化\n",
    "    initial = tf.truncated_normal(shape, mean = mean, stddev = stddev) #截断的正态分布\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#将风格，内容图像读取进来：\n",
    "def read_img(img_name):\n",
    "    #PIL里面的函数,读取img\n",
    "    img = Image.open(img_name) \n",
    "    \n",
    "    #变成numpy的矩阵\n",
    "    np_img = np.array(img) #（224,224,3)\n",
    "    \n",
    "    #矩阵是 (224, 224, 3), 需要改变成4维：\n",
    "    #因为只有一个样本，所以可以不用reshape，而是直接用[np_img]，将np_img包含在一个list [] 里面\n",
    "    #如此就直接多了一个维度，变成(1, 224, 224, 3)\n",
    "    np_img = np.asarray([np_img], dtype = np.int32)\n",
    "    \n",
    "    return np_img\n",
    "\n",
    "#计算Gram矩阵，用于风格损失：\n",
    "def gram_matrix(x):\n",
    "    \"\"\"Calculates gram matrix\n",
    "    Args:\n",
    "    - x: features extracted from VGG Net. shape: [1, width, height, channel]\n",
    "    \"\"\"\n",
    "    batch, w, h, ch = x.get_shape().as_list()\n",
    "    features = tf.reshape(x, [batch, h*w, ch]) #将w，h合成一个维度\n",
    "    \n",
    "    #针对channel，两两进行计算余弦相似度，得到gram matrix\n",
    "    #1. 我们的features的第一个维度batch，永远都等于1\n",
    "    #2. 我们的features的后两个维度，我们可以看成一个二维矩阵，假设叫A：行是h*w, 列是ch\n",
    "    #3. 计算所有channel两两之间的相似度，我们可以看做将A矩阵中，抽取两个列进行点乘\n",
    "    #4. 假设有k个channel，我们最后的gram matrix是k * k 矩阵，其中第i行，第j列代表第i个ch和第j个ch的相似度\n",
    "    #5. 计算ch * ch的点乘，可以用 (ch * hw) * (hw * ch)进行点乘，也就是features的转置，点乘features\n",
    "    #6. 矩阵乘法用tf.matmul(), 其中adjoin_a = True表示第一个参数要转置\n",
    "    #7. 担心点乘后值过大，所以我们还会除以一个常数：也就是各个维度的乘积，例如224*224*3\n",
    "    gram = tf.matmul(features, features, adjoint_a = True) \\\n",
    "            / tf.constant(ch * w * h, tf.float32)\n",
    "    return gram\n",
    "\n",
    "#初始化图像: mean是[0,255]的中间值\n",
    "result = initial_result((1, 224, 224, 3), 127.5, 20)\n",
    "\n",
    "#读取风格，内容图片\n",
    "content_val = read_img(content_img_path)\n",
    "style_val = read_img(style_img_path)\n",
    "\n",
    "#用feed_dict塞入，所以我们需要先创建placeholder\n",
    "content = tf.placeholder(tf.float32, shape = [1,224,224,3])\n",
    "style = tf.placeholder(tf.float32, shape = [1,224,224,3])\n",
    "\n",
    "#将三张图输入vggnet，然后提取特征\n",
    "data_dict = np.load(vgg16_npy_path, encoding = 'latin1').item()\n",
    "\n",
    "#创建三个vggnet，都是同样的参数\n",
    "#给内容图像创建vggnet\n",
    "vgg_for_content = VGGNet(data_dict)\n",
    "\n",
    "#给风格图像创建vggnet\n",
    "vgg_for_style = VGGNet(data_dict)\n",
    "\n",
    "#给结果图像创建vggnet\n",
    "vgg_for_result = VGGNet(data_dict)\n",
    "\n",
    "#调用build函数，完成vggnet构建\n",
    "#content和vgg_for_content进行关联\n",
    "vgg_for_content.build(content)\n",
    "vgg_for_style.build(style)\n",
    "vgg_for_result.build(result)\n",
    "\n",
    "#vggnet的每个层都可以进行特征提取\n",
    "#content：越底层提取的特征越清晰\n",
    "#以下是提取了5个特征，我们先注释掉后3个\n",
    "content_features = [\n",
    "    vgg_for_content.conv1_2,\n",
    "    vgg_for_content.conv2_2,\n",
    "    #vgg_for_content.conv3_3,\n",
    "    #vgg_for_content.conv4_3,\n",
    "    #vgg_for_content.conv5_3,\n",
    "]\n",
    "\n",
    "#给内容图像提取了哪些特征，就要对结果图像提取哪些特征\n",
    "result_content_features = [\n",
    "    vgg_for_result.conv1_2,\n",
    "    vgg_for_result.conv2_2,\n",
    "    #vgg_for_result.conv3_3,\n",
    "    #vgg_for_result.conv4_3,\n",
    "    #vgg_for_result.conv5_3,\n",
    "]\n",
    "\n",
    "#style: 越高层越好，所以注释掉底层\n",
    "style_features = [\n",
    "    #vgg_for_style.conv1_2,\n",
    "    #vgg_for_style.conv2_2,\n",
    "    #vgg_for_style.conv3_3,\n",
    "    vgg_for_style.conv4_3,\n",
    "    #vgg_for_style.conv5_3,\n",
    "]\n",
    "\n",
    "#给风格图像的风格特征计算gram矩阵：\n",
    "style_gram = [gram_matrix(feature) for feature in style_features]\n",
    "\n",
    "#给风格图像提取了哪些特征，就要对结果图像提取哪些特征\n",
    "result_style_features = [\n",
    "    #vgg_for_result.conv1_2,\n",
    "    #vgg_for_result.conv2_2,\n",
    "    #vgg_for_result.conv3_3,\n",
    "    vgg_for_result.conv4_3,\n",
    "    #vgg_for_result.conv5_3,\n",
    "]\n",
    "\n",
    "#给结果图像的风格特征计算gram矩阵：\n",
    "result_style_gram = [gram_matrix(feature) for feature in result_style_features]\n",
    "\n",
    "#计算内容损失 + 风格损失\n",
    "#1. 内容损失：是每一层损失的加和\n",
    "#zip: 将两个数组绑定在一起\n",
    "#例如：[1,2], [3,4] \n",
    "#zip([1,2], [3,4]) -> [(1,3), (2,4)]\n",
    "#两个list -> 一个list中两个pair：(1,3) 和 (2,4)\n",
    "#想象：将两个list竖着放，然后横着拿出来\n",
    "content_loss = tf.zeros(1, tf.float32) #一个数，是标量\n",
    "for c, c_ in zip(content_features, result_content_features):\n",
    "    #c, c_ = (content_features[0], result_content_features[0])\n",
    "    #c, c_ = (content_features[1], result_content_features[1])\n",
    "    #内容损失是提取的特征：c, c_的平方差，再求平均。\n",
    "    #平均是长宽高所有维度的平均，因为shape = [1, width, height, channel], 所以axis = [1,2,3]\n",
    "    content_loss += tf.reduce_mean((c - c_) ** 2, [1,2,3])\n",
    "    \n",
    "#2. 风格损失\n",
    "#将某一层的特征提取出来，会得到feature_map\n",
    "style_loss = tf.zeros(1, tf.float32)\n",
    "for s, s_ in zip(style_gram, result_style_gram):\n",
    "    #平方差损失函数，在1，2维度求均值。因为gram_matrix()已经将width和height降成1维。所以现在第零维是batch，第一维是w*h, 第二维度是ch\n",
    "    style_loss += tf.reduce_mean((s - s_) ** 2, [1,2])\n",
    "\n",
    "#loss加权\n",
    "loss = content_loss * lambda_c + style_loss * lambda_s\n",
    "    \n",
    "#给损失函数计算梯度\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 图像风格转换的训练流程图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1, loss_value: 66688.7891, content_loss: 348553.1250, style_loss:   6.3667\n",
      "step: 2, loss_value: 53495.6250, content_loss: 281534.5312, style_loss:   5.0684\n",
      "step: 3, loss_value: 47616.8594, content_loss: 249426.0938, style_loss:   4.5348\n",
      "step: 4, loss_value: 43397.3672, content_loss: 240139.0156, style_loss:   3.8767\n",
      "step: 5, loss_value: 38279.8125, content_loss: 239280.1562, style_loss:   2.8704\n",
      "step: 6, loss_value: 37569.5391, content_loss: 242960.9375, style_loss:   2.6547\n",
      "step: 7, loss_value: 34276.1836, content_loss: 245151.0625, style_loss:   1.9522\n",
      "step: 8, loss_value: 33399.0430, content_loss: 246349.9531, style_loss:   1.7528\n",
      "step: 9, loss_value: 32448.1309, content_loss: 245732.8438, style_loss:   1.5750\n",
      "step: 10, loss_value: 31447.1562, content_loss: 242432.1250, style_loss:   1.4408\n",
      "step: 11, loss_value: 30405.7031, content_loss: 237089.3438, style_loss:   1.3394\n",
      "step: 12, loss_value: 29232.3047, content_loss: 230494.7812, style_loss:   1.2366\n",
      "step: 13, loss_value: 28261.4375, content_loss: 222971.2656, style_loss:   1.1929\n",
      "step: 14, loss_value: 27019.2539, content_loss: 214522.7344, style_loss:   1.1134\n",
      "step: 15, loss_value: 25956.9180, content_loss: 205816.9531, style_loss:   1.0750\n",
      "step: 16, loss_value: 24871.8125, content_loss: 197148.6875, style_loss:   1.0314\n",
      "step: 17, loss_value: 23811.6914, content_loss: 188572.0625, style_loss:   0.9909\n",
      "step: 18, loss_value: 22816.7266, content_loss: 180274.9844, style_loss:   0.9578\n",
      "step: 19, loss_value: 21864.0312, content_loss: 172417.5469, style_loss:   0.9245\n",
      "step: 20, loss_value: 20945.3242, content_loss: 164926.1250, style_loss:   0.8905\n",
      "step: 21, loss_value: 20089.0430, content_loss: 157709.5312, style_loss:   0.8636\n",
      "step: 22, loss_value: 19294.5586, content_loss: 150744.1094, style_loss:   0.8440\n",
      "step: 23, loss_value: 18552.8438, content_loss: 144250.3125, style_loss:   0.8256\n",
      "step: 24, loss_value: 17864.8984, content_loss: 138285.2031, style_loss:   0.8073\n",
      "step: 25, loss_value: 17211.4316, content_loss: 132567.0312, style_loss:   0.7909\n",
      "step: 26, loss_value: 16596.9785, content_loss: 127343.7812, style_loss:   0.7725\n",
      "step: 27, loss_value: 16049.2324, content_loss: 122266.7969, style_loss:   0.7645\n",
      "step: 28, loss_value: 15599.5957, content_loss: 117750.0156, style_loss:   0.7649\n",
      "step: 29, loss_value: 15453.3643, content_loss: 112896.4688, style_loss:   0.8327\n",
      "step: 30, loss_value: 14845.8125, content_loss: 109357.0000, style_loss:   0.7820\n",
      "step: 31, loss_value: 14342.2080, content_loss: 105448.4844, style_loss:   0.7595\n",
      "step: 32, loss_value: 13802.8301, content_loss: 101982.9062, style_loss:   0.7209\n",
      "step: 33, loss_value: 13492.9121, content_loss: 98836.5703, style_loss:   0.7219\n",
      "step: 34, loss_value: 13261.7764, content_loss: 95673.5781, style_loss:   0.7389\n",
      "step: 35, loss_value: 12997.9287, content_loss: 93208.5859, style_loss:   0.7354\n",
      "step: 36, loss_value: 12913.4434, content_loss: 90247.8438, style_loss:   0.7777\n",
      "step: 37, loss_value: 12311.7646, content_loss: 88297.3750, style_loss:   0.6964\n",
      "step: 38, loss_value: 12054.4766, content_loss: 86178.6250, style_loss:   0.6873\n",
      "step: 39, loss_value: 11791.4082, content_loss: 83825.6484, style_loss:   0.6818\n",
      "step: 40, loss_value: 11653.5977, content_loss: 81958.0156, style_loss:   0.6916\n",
      "step: 41, loss_value: 11525.4746, content_loss: 79895.2891, style_loss:   0.7072\n",
      "step: 42, loss_value: 11206.2949, content_loss: 78393.5547, style_loss:   0.6734\n",
      "step: 43, loss_value: 11037.4922, content_loss: 76536.1328, style_loss:   0.6768\n",
      "step: 44, loss_value: 10676.9609, content_loss: 74966.1562, style_loss:   0.6361\n",
      "step: 45, loss_value: 10587.7930, content_loss: 73474.0156, style_loss:   0.6481\n",
      "step: 46, loss_value: 10356.2773, content_loss: 71799.9062, style_loss:   0.6353\n"
     ]
    }
   ],
   "source": [
    "#定义初始化op\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op) #初始化变量，在图像风格转换中，变量只有一个：结果图像（对结果图像进行梯度下降）\n",
    "    \n",
    "    #开始训练：\n",
    "    for step in range(num_steps):\n",
    "        #最后一个_是因为要训练\n",
    "        loss_value, content_loss_value, style_loss_value, _ \\\n",
    "            = sess.run([loss, content_loss, style_loss, train_op],\n",
    "                        feed_dict = {\n",
    "                            content: content_val,\n",
    "                            style: style_val,\n",
    "                        })\n",
    "        print('step: %d, loss_value: %8.4f, content_loss: %8.4f, style_loss: %8.4f' \\\n",
    "             % (step+1, \n",
    "                loss_value[0],  #之所以是[0], 是因为我们是四维的，第一个维度只有一个元素，所以取[0]\n",
    "                content_loss_value[0], \n",
    "                style_loss_value[0]))\n",
    "        \n",
    "        #获得每一步的结果图像，输出到dir中\n",
    "        result_img_path = os.path.join(\n",
    "            output_dir, 'result-%05d.jpg' % (step + 1))\n",
    "        \n",
    "        #将参数值，也就是结果图像的变量值，取出来\n",
    "        #result是之前定义的随机噪声 result = initial_result((1, 224, 224, 3), 127.5, 20)\n",
    "        result_val = result.eval(sess)[0] #因为第一个维度是只有一个元素，所以取出用[0]\n",
    "        result_val = np.clip(result_val, 0, 255) #clip的作用：裁剪，将小于0的设成0，大于255的设置成255\n",
    "\n",
    "        #之前是float，现在转换成int\n",
    "        img_arr = np.asarray(result_val, np.uint8) \n",
    "        \n",
    "        #用PIL转换成图片\n",
    "        img = Image.fromarray(img_arr) #可以将numpy数组转换成图片\n",
    "        img.save(result_img_path)\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为何之前设置这样的风格系数，内容系数\n",
    "1. 首先我们的style loss的计算中，gram matrix需要除以一个比较大的数tf.constant(ch * w * h, tf.float32)，导致了style loss比较小\n",
    "2. gram = tf.matmul(features, features, adjoint_a = True) \\\n",
    "            / tf.constant(ch * w * h, tf.float32)\n",
    "3. 既然style loss小，content loss大，为了均衡两者，所以一个乘500，一个乘0.1\n",
    "4. lambda_c = 0.1 #内容， lambda_s = 500 #风格\n",
    "5. style loss: 1.7 * 500 = 850\n",
    "6. content loss: 12997 * 0.1 = 1299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
