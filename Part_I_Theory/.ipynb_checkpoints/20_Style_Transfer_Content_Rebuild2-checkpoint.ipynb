{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建计算图\n",
    "修改：我们做风格转换不需要最后的fc，并且fc含有参数多，费时，所以我们这里注释掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image #图像处理库\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/bessie/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG中自带常量，VGG有将图片进行预处理，其中一个步骤是normalization:\n",
    "#减去image_net的RGB通道的各个均值\n",
    "VGG_MEAN = [103.939, 116.779, 123.68] #在vggnet的code中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNet:\n",
    "    \"\"\"Builds VGG-16 net structure,\n",
    "        load parameters from pre-trained models.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dict):\n",
    "        self.data_dict = data_dict\n",
    "        \n",
    "    def get_conv_filter(self, name): #抽取卷积参数\n",
    "        \"\"\"eg. conv1_1 = data_dict['conv1_1']\"\"\"\n",
    "        #tf.constant() #因为模型是预处理好的，所以我们不会改变参数，所以定义为常量。\n",
    "        #另一个方法：可以设置成trainable = False\n",
    "        return tf.constant(self.data_dict[name][0], name = 'conv') #这里应该是w,b中的w\n",
    "    \n",
    "    def get_fc_weight(self, name):\n",
    "        return tf.constant(self.data_dict[name][0], name = 'fc') #这里应该是w,b中的w\n",
    "    \n",
    "    def get_bias(self, name):\n",
    "        return tf.constant(self.data_dict[name][1], name = 'bias') #这里应该是w,b中的b\n",
    "    \n",
    "    #创建卷积层，池化层，全连接层\n",
    "    def conv_layer(self, x, name):\n",
    "        \"\"\"Builds convolution layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            #加上name_scope是命名规范：\n",
    "            #1. 防止命名冲突\n",
    "            #2. tensorboard打印名字更加清晰规范\n",
    "            conv_w = self.get_conv_filter(name)\n",
    "            conv_b = self.get_bias(name)\n",
    "            \n",
    "            #现在不再使用tf.layers.conv2d(),因为我们已经有了pre-trained的参数\n",
    "            #现在使用更基础的api: tf.nn.conv2d()\n",
    "            h = tf.nn.conv2d(x, conv_w, [1,1,1,1], padding = 'SAME') #x是input，[1,1,1,1]是strides步长，因为这里x是四维，所以我们输入四个数\n",
    "            h = tf.nn.bias_add(h, conv_b)\n",
    "            \n",
    "            #激活函数\n",
    "            h = tf.nn.relu(h)\n",
    "            return h\n",
    "        \n",
    "    #也是使用tf.nn.max_pool()而不是tf.layers.max_pooling2d\n",
    "    def pooling_layer(self, x, name):\n",
    "        \"\"\"Builds pooling layer.\"\"\"\n",
    "        return tf.nn.max_pool(x,\n",
    "                                 ksize = [1,2,2,1], #因为是按照长和宽来池化，所以是中间两个维度是2，其余维度是1\n",
    "                                 strides = [1,2,2,1], \n",
    "                                 padding = 'SAME', \n",
    "                                 name = name) \n",
    "    \n",
    "    \n",
    "    def fc_layer(self, x, name, activation = tf.nn.relu):\n",
    "        \"\"\"Builds fully-connected layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            fc_w = self.get_fc_weight(name)\n",
    "            fc_b = self.get_bias(name)\n",
    "            h = tf.matmul(x, fc_w) #让输入x与w进行操作\n",
    "            h = tf.nn.bias_add(h, fc_b)\n",
    "            if activation == None:\n",
    "                return h\n",
    "            else:\n",
    "                return activation(h)\n",
    "            \n",
    "    \n",
    "    #创建展平功能，展平后输入给全连接层：做的是reshape操作，我们需要知道reshape之后的size有多大\n",
    "    #展平之后，需要的长宽厚的乘积\n",
    "    def flatten_layer(self, x, name):\n",
    "        \"\"\"Builds flatten layer.\"\"\"\n",
    "        with tf.name_scope(name):\n",
    "            #[batch_size, img_width, img_height, channel]\n",
    "            x_shape = x.get_shape().as_list()\n",
    "            dim = 1\n",
    "            for d in x_shape[1:]:\n",
    "                dim *= d\n",
    "            x = tf.reshape(x, [-1, dim]) #这里的-1，是reshape推断出来的，也是我们的batch_size, 你也可以写成[x_shape[0], dim]\n",
    "            return x\n",
    "       \n",
    "    #建立vgg\n",
    "    #我们现在就要做图像的风格转换，需要的图片只有一个，所以第一个维度是1\n",
    "    #vggnet的设置中，图像大小是224*224\n",
    "    def build(self, x_rgb):\n",
    "        \"\"\"BUild VGG16 network structure.\n",
    "        Args:\n",
    "        - x_rgb: eg. [1, 224, 224, 3]\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        print(\"Building model...\")\n",
    "        \n",
    "        #每个通道减去均值VGG_MEAN，先拆分通道\n",
    "        #复习：tf.split() 之前用于：深度可分离卷积，数据增强\n",
    "        r, g, b = tf.split(x_rgb, [1,1,1], axis = 3) #切分成三通道：[1,1,1]\n",
    "        \n",
    "        #去除均值后，需要合并。这里注意vggnet输入的通道顺序是BGR\n",
    "        #意味着之前写的VGG_MEAN的三个数分别是 BGR 的均值\n",
    "        x_bgr = tf.concat([b - VGG_MEAN[0], \n",
    "                           g - VGG_MEAN[1],\n",
    "                           r - VGG_MEAN[2]],\n",
    "                          axis = 3) #在第四个维度，channel上合并\n",
    "        \n",
    "        #预处理之后，判断一下我们的维度是 224*224*3\n",
    "        assert x_bgr.get_shape().as_list()[1:] == [224,224,3]\n",
    "        \n",
    "        #构建前两个卷积层：\n",
    "        #vgg16：\n",
    "        #第一个结构(stage)：两个卷积层 -> 一个池化层\n",
    "        #第二个结构：两个卷积层 -> 一个池化层\n",
    "        #第3个结构：3个卷积层 -> 一个池化层\n",
    "        #第4个结构：3个卷积层 -> 一个池化层\n",
    "        #第5个结构：3个卷积层 -> 一个池化层\n",
    "        #第6个结构：3个全连接层\n",
    "        #2*2 + 3*3 + 3 = 4 + 9 + 3 = 16, 也就是vgg16 \n",
    "        \n",
    "        ##注意：self.conv_layer(xx,yy)第二个参数的名字必须是data_dict.keys()中的\n",
    "        #dict_keys(['conv5_1', 'fc6', 'conv5_3', 'conv5_2', 'fc8', 'fc7', 'conv4_1', 'conv4_2', 'conv4_3', 'conv3_3', 'conv3_2', 'conv3_1', 'conv1_1', 'conv1_2', 'conv2_2', 'conv2_1'])\n",
    "        #我们将每一个层，设置成了成员变量, eg. self.conv1_1, 可能会用其中的某一层计算风格损失或者内容损失，设置成成员变量我们以后可以方便使用\n",
    "        self.conv1_1 = self.conv_layer(x_bgr, 'conv1_1')\n",
    "        self.conv1_2 = self.conv_layer(self.conv1_1, 'conv1_2')\n",
    "        self.pool1 = self.pooling_layer(self.conv1_2, 'pool1') #pool1因为不是data_dict里面预处理好的，所以我们可以随意命名, 例如pool1。并且可以不用将它设置成成员函数，只不过这里为了统一起见\n",
    "        \n",
    "        self.conv2_1 = self.conv_layer(self.pool1, 'conv2_1')\n",
    "        self.conv2_2 = self.conv_layer(self.conv2_1, 'conv2_2')\n",
    "        self.pool2 = self.pooling_layer(self.conv2_2, 'pool2')\n",
    "        \n",
    "        self.conv3_1 = self.conv_layer(self.pool2, 'conv3_1')\n",
    "        self.conv3_2 = self.conv_layer(self.conv3_1, 'conv3_2')\n",
    "        self.conv3_3 = self.conv_layer(self.conv3_2, 'conv3_3')\n",
    "        self.pool3 = self.pooling_layer(self.conv3_3, 'pool3')\n",
    "        \n",
    "        self.conv4_1 = self.conv_layer(self.pool3, 'conv4_1')\n",
    "        self.conv4_2 = self.conv_layer(self.conv4_1, 'conv4_2')\n",
    "        self.conv4_3 = self.conv_layer(self.conv4_2, 'conv4_3')\n",
    "        self.pool4 = self.pooling_layer(self.conv4_3, 'pool4')\n",
    "        \n",
    "        self.conv5_1 = self.conv_layer(self.pool4, 'conv5_1')\n",
    "        self.conv5_2 = self.conv_layer(self.conv5_1, 'conv5_2')\n",
    "        self.conv5_3 = self.conv_layer(self.conv5_2, 'conv5_3')\n",
    "        self.pool5 = self.pooling_layer(self.conv5_3, 'pool5')\n",
    "        \n",
    "        '''\n",
    "        #展开 -> 全连接\n",
    "        self.flatten5 = self.flatten_layer(self.pool5, 'flatten')\n",
    "        self.fc6 = self.fc_layer(self.flatten5, 'fc6')\n",
    "        self.fc7 = self.fc_layer(self.fc6, 'fc7')\n",
    "        \n",
    "        #最后的fc8输出1k个值，给softmax()去计算概率分布\n",
    "        #所以fc8不需要activation\n",
    "        self.fc8 = self.fc_layer(self.fc7, 'fc8', activation = None)\n",
    "        \n",
    "        #计算softmax\n",
    "        self.prob = tf.nn.softmax(self.fc8, name = 'prob')\n",
    "        '''\n",
    "        \n",
    "        print(\"Building model finished: %4ds\" % (time.time() - start_time))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试我们的VGGNet 类是否成功："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Building model finished:    0s\n"
     ]
    }
   ],
   "source": [
    "#加载vgg16\n",
    "vgg16_npy_path = '../../../other_datasets/vgg16.npy'\n",
    "data_dict = np.load(vgg16_npy_path, encoding = 'latin1').item() #加item()是为了创建成字典\n",
    "\n",
    "vgg16_for_result = VGGNet(data_dict)\n",
    "content = tf.placeholder(tf.float32, shape = [1,224,224,3])\n",
    "vgg16_for_result.build(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将图片都设置为大小是 224 * 224 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 224, 224, 3)\n",
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "content_img_path = './others/citi.JPG'\n",
    "\n",
    "img_string = tf.read_file(content_img_path)\n",
    "img_decoded = tf.image.decode_image(img_string)\n",
    "\n",
    "sess = tf.Session()\n",
    "#用sess执行这个图\n",
    "img_decoded_val = sess.run(img_decoded)\n",
    "\n",
    "#因为tf.image.resize_bicubic()处理的是batch数据，所以需要4维输入，第一维就是图片数\n",
    "shape = img_decoded_val.shape\n",
    "img_decoded = tf.reshape(img_decoded, [1, shape[0], shape[1], shape[2]])\n",
    "\n",
    "#在img_decoded上面加工：\n",
    "resize_img = tf.image.resize_bicubic(img_decoded, [224, 224])\n",
    "\n",
    "sess = tf.Session()\n",
    "img_decoded_val = sess.run(resize_img)\n",
    "\n",
    "#需要将4维改成3维：\n",
    "print(img_decoded_val.shape)#(1, 224, 224, 3)\n",
    "img_decoded_val = img_decoded_val.reshape((224, 224, 3))\n",
    "print(img_decoded_val.shape)\n",
    "\n",
    "#需要做类型变换：从[0,1]之间的小数float 改为 [0,255]integer \n",
    "img_decoded_val = np.asarray(img_decoded_val, np.uint8)\n",
    "\n",
    "resize_img_path = os.path.join('./others', 'resized_citi.jpg')\n",
    "\n",
    "img = Image.fromarray(img_decoded_val) #可以将numpy数组转换成图片\n",
    "img.save(resize_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 224, 224, 3)\n",
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "style_img_path = './others/starry_night.jpeg'\n",
    "\n",
    "img_string = tf.read_file(style_img_path)\n",
    "img_decoded = tf.image.decode_image(img_string)\n",
    "\n",
    "sess = tf.Session()\n",
    "#用sess执行这个图\n",
    "img_decoded_val = sess.run(img_decoded)\n",
    "\n",
    "#因为tf.image.resize_bicubic()处理的是batch数据，所以需要4维输入，第一维就是图片数\n",
    "shape = img_decoded_val.shape\n",
    "img_decoded = tf.reshape(img_decoded, [1, shape[0], shape[1], shape[2]])\n",
    "\n",
    "#在img_decoded上面加工：\n",
    "resize_img = tf.image.resize_bicubic(img_decoded, [224, 224])\n",
    "\n",
    "sess = tf.Session()\n",
    "img_decoded_val = sess.run(resize_img)\n",
    "\n",
    "#需要将4维改成3维：\n",
    "print(img_decoded_val.shape)##(1, 224, 224, 3)\n",
    "img_decoded_val = img_decoded_val.reshape((224, 224, 3))\n",
    "print(img_decoded_val.shape)\n",
    "\n",
    "#需要做类型变换：从[0,1]之间的小数float 改为 [0,255]integer \n",
    "img_decoded_val = np.asarray(img_decoded_val, np.uint8)\n",
    "\n",
    "resize_img_path = os.path.join('./others', 'resized_starry_night.jpg')\n",
    "\n",
    "img = Image.fromarray(img_decoded_val) #可以将numpy数组转换成图片\n",
    "img.save(resize_img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始风格转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_img_path = './others/resized_citi.JPG'\n",
    "style_img_path = './others/resized_starry_night.jpg'\n",
    "\n",
    "#训练次数\n",
    "num_steps = 100\n",
    "\n",
    "#学习率\n",
    "learning_rate = 10\n",
    "\n",
    "#风格系数，内容系数\n",
    "lambda_c = 1 #内容\n",
    "lambda_s = 0 #风格\n",
    "\n",
    "#输出文件夹, 每一步都有新图像，将所有图像输入这个文件夹\n",
    "output_dir = './content_rebuild_output_1'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Building model finished:    0s\n",
      "Building model...\n",
      "Building model finished:    0s\n",
      "Building model...\n",
      "Building model finished:    0s\n"
     ]
    }
   ],
   "source": [
    "#定义随机的初始图片：\n",
    "def initial_result(shape, mean, stddev):\n",
    "    #用正态分布初始化\n",
    "    initial = tf.truncated_normal(shape, mean = mean, stddev = stddev) #截断的正态分布\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#将风格，内容图像读取进来：\n",
    "def read_img(img_name):\n",
    "    #PIL里面的函数,读取img\n",
    "    img = Image.open(img_name) \n",
    "    \n",
    "    #变成numpy的矩阵\n",
    "    np_img = np.array(img) #（224,224,3)\n",
    "    \n",
    "    #矩阵是 (224, 224, 3), 需要改变成4维：\n",
    "    #因为只有一个样本，所以可以不用reshape，而是直接用[np_img]，将np_img包含在一个list [] 里面\n",
    "    #如此就直接多了一个维度，变成(1, 224, 224, 3)\n",
    "    np_img = np.asarray([np_img], dtype = np.int32)\n",
    "    \n",
    "    return np_img\n",
    "\n",
    "#计算Gram矩阵，用于风格损失：\n",
    "def gram_matrix(x):\n",
    "    \"\"\"Calculates gram matrix\n",
    "    Args:\n",
    "    - x: features extracted from VGG Net. shape: [1, width, height, channel]\n",
    "    \"\"\"\n",
    "    batch, w, h, ch = x.get_shape().as_list()\n",
    "    features = tf.reshape(x, [batch, h*w, ch]) #将w，h合成一个维度\n",
    "    \n",
    "    #针对channel，两两进行计算余弦相似度，得到gram matrix\n",
    "    #1. 我们的features的第一个维度batch，永远都等于1\n",
    "    #2. 我们的features的后两个维度，我们可以看成一个二维矩阵，假设叫A：行是h*w, 列是ch\n",
    "    #3. 计算所有channel两两之间的相似度，我们可以看做将A矩阵中，抽取两个列进行点乘\n",
    "    #4. 假设有k个channel，我们最后的gram matrix是k * k 矩阵，其中第i行，第j列代表第i个ch和第j个ch的相似度\n",
    "    #5. 计算ch * ch的点乘，可以用 (ch * hw) * (hw * ch)进行点乘，也就是features的转置，点乘features\n",
    "    #6. 矩阵乘法用tf.matmul(), 其中adjoin_a = True表示第一个参数要转置\n",
    "    #7. 担心点乘后值过大，所以我们还会除以一个常数：也就是各个维度的乘积，例如224*224*3\n",
    "    gram = tf.matmul(features, features, adjoint_a = True) \\\n",
    "            / tf.constant(ch * w * h, tf.float32)\n",
    "    return gram\n",
    "\n",
    "#初始化图像: mean是[0,255]的中间值\n",
    "result = initial_result((1, 224, 224, 3), 127.5, 20)\n",
    "\n",
    "#读取风格，内容图片\n",
    "content_val = read_img(content_img_path)\n",
    "style_val = read_img(style_img_path)\n",
    "\n",
    "#用feed_dict塞入，所以我们需要先创建placeholder\n",
    "content = tf.placeholder(tf.float32, shape = [1,224,224,3])\n",
    "style = tf.placeholder(tf.float32, shape = [1,224,224,3])\n",
    "\n",
    "#将三张图输入vggnet，然后提取特征\n",
    "data_dict = np.load(vgg16_npy_path, encoding = 'latin1').item()\n",
    "\n",
    "#创建三个vggnet，都是同样的参数\n",
    "#给内容图像创建vggnet\n",
    "vgg_for_content = VGGNet(data_dict)\n",
    "\n",
    "#给风格图像创建vggnet\n",
    "vgg_for_style = VGGNet(data_dict)\n",
    "\n",
    "#给结果图像创建vggnet\n",
    "vgg_for_result = VGGNet(data_dict)\n",
    "\n",
    "#调用build函数，完成vggnet构建\n",
    "#content和vgg_for_content进行关联\n",
    "vgg_for_content.build(content)\n",
    "vgg_for_style.build(style)\n",
    "vgg_for_result.build(result)\n",
    "\n",
    "#vggnet的每个层都可以进行特征提取\n",
    "#content：越底层提取的特征越清晰\n",
    "#以下是提取了5个特征，我们先注释掉后3个\n",
    "content_features = [\n",
    "    vgg_for_content.conv1_2,\n",
    "    vgg_for_content.conv2_2,\n",
    "    #vgg_for_content.conv3_3,\n",
    "    #vgg_for_content.conv4_3,\n",
    "    #vgg_for_content.conv5_3,\n",
    "]\n",
    "\n",
    "#给内容图像提取了哪些特征，就要对结果图像提取哪些特征\n",
    "result_content_features = [\n",
    "    vgg_for_result.conv1_2,\n",
    "    vgg_for_result.conv2_2,\n",
    "    #vgg_for_result.conv3_3,\n",
    "    #vgg_for_result.conv4_3,\n",
    "    #vgg_for_result.conv5_3,\n",
    "]\n",
    "\n",
    "#style: 越高层越好，所以注释掉底层\n",
    "style_features = [\n",
    "    #vgg_for_style.conv1_2,\n",
    "    #vgg_for_style.conv2_2,\n",
    "    #vgg_for_style.conv3_3,\n",
    "    vgg_for_style.conv4_3,\n",
    "    #vgg_for_style.conv5_3,\n",
    "]\n",
    "\n",
    "#给风格图像的风格特征计算gram矩阵：\n",
    "style_gram = [gram_matrix(feature) for feature in style_features]\n",
    "\n",
    "#给风格图像提取了哪些特征，就要对结果图像提取哪些特征\n",
    "result_style_features = [\n",
    "    #vgg_for_result.conv1_2,\n",
    "    #vgg_for_result.conv2_2,\n",
    "    #vgg_for_result.conv3_3,\n",
    "    vgg_for_result.conv4_3,\n",
    "    #vgg_for_result.conv5_3,\n",
    "]\n",
    "\n",
    "#给结果图像的风格特征计算gram矩阵：\n",
    "result_style_gram = [gram_matrix(feature) for feature in result_style_features]\n",
    "\n",
    "#计算内容损失 + 风格损失\n",
    "#1. 内容损失：是每一层损失的加和\n",
    "#zip: 将两个数组绑定在一起\n",
    "#例如：[1,2], [3,4] \n",
    "#zip([1,2], [3,4]) -> [(1,3), (2,4)]\n",
    "#两个list -> 一个list中两个pair：(1,3) 和 (2,4)\n",
    "#想象：将两个list竖着放，然后横着拿出来\n",
    "content_loss = tf.zeros(1, tf.float32) #一个数，是标量\n",
    "for c, c_ in zip(content_features, result_content_features):\n",
    "    #c, c_ = (content_features[0], result_content_features[0])\n",
    "    #c, c_ = (content_features[1], result_content_features[1])\n",
    "    #内容损失是提取的特征：c, c_的平方差，再求平均。\n",
    "    #平均是长宽高所有维度的平均，因为shape = [1, width, height, channel], 所以axis = [1,2,3]\n",
    "    content_loss += tf.reduce_mean((c - c_) ** 2, [1,2,3])\n",
    "    \n",
    "#2. 风格损失\n",
    "#将某一层的特征提取出来，会得到feature_map\n",
    "style_loss = tf.zeros(1, tf.float32)\n",
    "for s, s_ in zip(style_gram, result_style_gram):\n",
    "    #平方差损失函数，在1，2维度求均值。因为gram_matrix()已经将width和height降成1维。所以现在第零维是batch，第一维是w*h, 第二维度是ch\n",
    "    style_loss += tf.reduce_mean((s - s_) ** 2, [1,2])\n",
    "\n",
    "#loss加权\n",
    "loss = content_loss * lambda_c + style_loss * lambda_s\n",
    "    \n",
    "#给损失函数计算梯度\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 图像风格转换的训练流程图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1, loss_value: 348465.8750, content_loss: 348465.8750, style_loss:   6.3577\n",
      "step: 2, loss_value: 260041.7031, content_loss: 260041.7031, style_loss:   5.7416\n",
      "step: 3, loss_value: 197191.3281, content_loss: 197191.3281, style_loss:   4.8199\n",
      "step: 4, loss_value: 156342.5625, content_loss: 156342.5625, style_loss:   4.3291\n",
      "step: 5, loss_value: 130009.8672, content_loss: 130009.8672, style_loss:   4.5116\n",
      "step: 6, loss_value: 112438.7422, content_loss: 112438.7422, style_loss:   5.4559\n",
      "step: 7, loss_value: 99564.7344, content_loss: 99564.7344, style_loss:   6.9495\n",
      "step: 8, loss_value: 88984.3594, content_loss: 88984.3594, style_loss:   8.6073\n",
      "step: 9, loss_value: 79781.4688, content_loss: 79781.4688, style_loss:  10.0585\n",
      "step: 10, loss_value: 71606.4922, content_loss: 71606.4922, style_loss:  11.2529\n",
      "step: 11, loss_value: 64481.5312, content_loss: 64481.5312, style_loss:  12.1636\n",
      "step: 12, loss_value: 58443.3750, content_loss: 58443.3750, style_loss:  12.7660\n",
      "step: 13, loss_value: 53431.8359, content_loss: 53431.8359, style_loss:  13.0940\n",
      "step: 14, loss_value: 49322.2500, content_loss: 49322.2500, style_loss:  13.1696\n",
      "step: 15, loss_value: 45847.1172, content_loss: 45847.1172, style_loss:  13.1170\n",
      "step: 16, loss_value: 42626.2539, content_loss: 42626.2539, style_loss:  13.0667\n",
      "step: 17, loss_value: 39485.4922, content_loss: 39485.4922, style_loss:  12.9481\n",
      "step: 18, loss_value: 36434.5078, content_loss: 36434.5078, style_loss:  12.7792\n",
      "step: 19, loss_value: 33557.2969, content_loss: 33557.2969, style_loss:  12.5606\n",
      "step: 20, loss_value: 30961.7637, content_loss: 30961.7637, style_loss:  12.3145\n",
      "step: 21, loss_value: 28655.1543, content_loss: 28655.1543, style_loss:  12.0479\n",
      "step: 22, loss_value: 26587.2715, content_loss: 26587.2715, style_loss:  11.7847\n",
      "step: 23, loss_value: 24692.0898, content_loss: 24692.0898, style_loss:  11.4853\n",
      "step: 24, loss_value: 22933.5820, content_loss: 22933.5820, style_loss:  11.1661\n",
      "step: 25, loss_value: 21286.1562, content_loss: 21286.1562, style_loss:  10.9085\n",
      "step: 26, loss_value: 19759.2656, content_loss: 19759.2656, style_loss:  10.7520\n",
      "step: 27, loss_value: 18352.8281, content_loss: 18352.8281, style_loss:  10.6589\n",
      "step: 28, loss_value: 17066.5156, content_loss: 17066.5156, style_loss:  10.6051\n",
      "step: 29, loss_value: 15904.3477, content_loss: 15904.3477, style_loss:  10.5793\n",
      "step: 30, loss_value: 14867.3838, content_loss: 14867.3838, style_loss:  10.5627\n",
      "step: 31, loss_value: 13920.7715, content_loss: 13920.7715, style_loss:  10.5713\n",
      "step: 32, loss_value: 13036.6836, content_loss: 13036.6836, style_loss:  10.6219\n",
      "step: 33, loss_value: 12201.1797, content_loss: 12201.1797, style_loss:  10.6977\n",
      "step: 34, loss_value: 11413.9883, content_loss: 11413.9883, style_loss:  10.7931\n",
      "step: 35, loss_value: 10684.6309, content_loss: 10684.6309, style_loss:  10.8849\n",
      "step: 36, loss_value: 10008.7344, content_loss: 10008.7344, style_loss:  10.9354\n",
      "step: 37, loss_value: 9384.9336, content_loss: 9384.9336, style_loss:  10.9710\n",
      "step: 38, loss_value: 8806.8350, content_loss: 8806.8350, style_loss:  10.9944\n",
      "step: 39, loss_value: 8278.1738, content_loss: 8278.1738, style_loss:  11.0129\n",
      "step: 40, loss_value: 7798.1025, content_loss: 7798.1025, style_loss:  11.0417\n",
      "step: 41, loss_value: 7354.1992, content_loss: 7354.1992, style_loss:  11.0831\n",
      "step: 42, loss_value: 6934.1411, content_loss: 6934.1411, style_loss:  11.1295\n",
      "step: 43, loss_value: 6535.6338, content_loss: 6535.6338, style_loss:  11.1924\n",
      "step: 44, loss_value: 6160.0098, content_loss: 6160.0098, style_loss:  11.2556\n",
      "step: 45, loss_value: 5806.0225, content_loss: 5806.0225, style_loss:  11.3151\n",
      "step: 46, loss_value: 5474.2642, content_loss: 5474.2642, style_loss:  11.3636\n",
      "step: 47, loss_value: 5160.7168, content_loss: 5160.7168, style_loss:  11.4036\n",
      "step: 48, loss_value: 4858.9072, content_loss: 4858.9072, style_loss:  11.4466\n",
      "step: 49, loss_value: 4568.2583, content_loss: 4568.2583, style_loss:  11.4805\n",
      "step: 50, loss_value: 4293.8613, content_loss: 4293.8613, style_loss:  11.4947\n",
      "step: 51, loss_value: 4041.1646, content_loss: 4041.1646, style_loss:  11.5083\n",
      "step: 52, loss_value: 3807.9648, content_loss: 3807.9648, style_loss:  11.5267\n",
      "step: 53, loss_value: 3590.0576, content_loss: 3590.0576, style_loss:  11.5405\n",
      "step: 54, loss_value: 3388.6338, content_loss: 3388.6338, style_loss:  11.5260\n",
      "step: 55, loss_value: 3200.0276, content_loss: 3200.0276, style_loss:  11.5093\n",
      "step: 56, loss_value: 3021.4189, content_loss: 3021.4189, style_loss:  11.5173\n",
      "step: 57, loss_value: 2852.6035, content_loss: 2852.6035, style_loss:  11.5548\n",
      "step: 58, loss_value: 2694.2314, content_loss: 2694.2314, style_loss:  11.6010\n",
      "step: 59, loss_value: 2544.5776, content_loss: 2544.5776, style_loss:  11.6252\n",
      "step: 60, loss_value: 2404.5879, content_loss: 2404.5879, style_loss:  11.6358\n",
      "step: 61, loss_value: 2275.2021, content_loss: 2275.2021, style_loss:  11.6535\n",
      "step: 62, loss_value: 2154.5090, content_loss: 2154.5090, style_loss:  11.6720\n",
      "step: 63, loss_value: 2040.2533, content_loss: 2040.2533, style_loss:  11.6762\n",
      "step: 64, loss_value: 1932.9531, content_loss: 1932.9531, style_loss:  11.6769\n",
      "step: 65, loss_value: 1833.8954, content_loss: 1833.8954, style_loss:  11.6865\n",
      "step: 66, loss_value: 1741.7820, content_loss: 1741.7820, style_loss:  11.6994\n",
      "step: 67, loss_value: 1655.4921, content_loss: 1655.4921, style_loss:  11.7111\n",
      "step: 68, loss_value: 1574.9822, content_loss: 1574.9822, style_loss:  11.7160\n",
      "step: 69, loss_value: 1498.5314, content_loss: 1498.5314, style_loss:  11.7165\n",
      "step: 70, loss_value: 1427.1367, content_loss: 1427.1367, style_loss:  11.7185\n",
      "step: 71, loss_value: 1360.3049, content_loss: 1360.3049, style_loss:  11.7230\n",
      "step: 72, loss_value: 1297.4619, content_loss: 1297.4619, style_loss:  11.7297\n",
      "step: 73, loss_value: 1238.2162, content_loss: 1238.2162, style_loss:  11.7288\n",
      "step: 74, loss_value: 1182.5393, content_loss: 1182.5393, style_loss:  11.7237\n",
      "step: 75, loss_value: 1130.6704, content_loss: 1130.6704, style_loss:  11.7305\n",
      "step: 76, loss_value: 1081.8809, content_loss: 1081.8809, style_loss:  11.7454\n",
      "step: 77, loss_value: 1036.1615, content_loss: 1036.1615, style_loss:  11.7628\n",
      "step: 78, loss_value: 993.1516, content_loss: 993.1516, style_loss:  11.7760\n",
      "step: 79, loss_value: 952.8871, content_loss: 952.8871, style_loss:  11.7845\n",
      "step: 80, loss_value: 915.4937, content_loss: 915.4937, style_loss:  11.7854\n",
      "step: 81, loss_value: 880.4205, content_loss: 880.4205, style_loss:  11.7824\n",
      "step: 82, loss_value: 847.3726, content_loss: 847.3726, style_loss:  11.7750\n",
      "step: 83, loss_value: 815.3045, content_loss: 815.3045, style_loss:  11.7698\n",
      "step: 84, loss_value: 784.8242, content_loss: 784.8242, style_loss:  11.7681\n",
      "step: 85, loss_value: 756.3940, content_loss: 756.3940, style_loss:  11.7728\n",
      "step: 86, loss_value: 730.3307, content_loss: 730.3307, style_loss:  11.7762\n",
      "step: 87, loss_value: 706.1594, content_loss: 706.1594, style_loss:  11.7817\n",
      "step: 88, loss_value: 683.2010, content_loss: 683.2010, style_loss:  11.7854\n",
      "step: 89, loss_value: 661.3474, content_loss: 661.3474, style_loss:  11.7920\n",
      "step: 90, loss_value: 639.9552, content_loss: 639.9552, style_loss:  11.8002\n",
      "step: 91, loss_value: 619.4828, content_loss: 619.4828, style_loss:  11.8015\n",
      "step: 92, loss_value: 600.2253, content_loss: 600.2253, style_loss:  11.8062\n",
      "step: 93, loss_value: 582.6084, content_loss: 582.6084, style_loss:  11.8043\n",
      "step: 94, loss_value: 566.1437, content_loss: 566.1437, style_loss:  11.8108\n",
      "step: 95, loss_value: 550.8162, content_loss: 550.8162, style_loss:  11.8168\n",
      "step: 96, loss_value: 536.0154, content_loss: 536.0154, style_loss:  11.8258\n",
      "step: 97, loss_value: 522.1058, content_loss: 522.1058, style_loss:  11.8347\n"
     ]
    }
   ],
   "source": [
    "#定义初始化op\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op) #初始化变量，在图像风格转换中，变量只有一个：结果图像（对结果图像进行梯度下降）\n",
    "    \n",
    "    #开始训练：\n",
    "    for step in range(num_steps):\n",
    "        #最后一个_是因为要训练\n",
    "        loss_value, content_loss_value, style_loss_value, _ \\\n",
    "            = sess.run([loss, content_loss, style_loss, train_op],\n",
    "                        feed_dict = {\n",
    "                            content: content_val,\n",
    "                            style: style_val,\n",
    "                        })\n",
    "        print('step: %d, loss_value: %8.4f, content_loss: %8.4f, style_loss: %8.4f' \\\n",
    "             % (step+1, \n",
    "                loss_value[0],  #之所以是[0], 是因为我们是四维的，第一个维度只有一个元素，所以取[0]\n",
    "                content_loss_value[0], \n",
    "                style_loss_value[0]))\n",
    "        \n",
    "        #获得每一步的结果图像，输出到dir中\n",
    "        result_img_path = os.path.join(\n",
    "            output_dir, 'result-%05d.jpg' % (step + 1))\n",
    "        \n",
    "        #将参数值，也就是结果图像的变量值，取出来\n",
    "        #result是之前定义的随机噪声 result = initial_result((1, 224, 224, 3), 127.5, 20)\n",
    "        result_val = result.eval(sess)[0] #因为第一个维度是只有一个元素，所以取出用[0]\n",
    "        result_val = np.clip(result_val, 0, 255) #clip的作用：裁剪，将小于0的设成0，大于255的设置成255\n",
    "\n",
    "        #之前是float，现在转换成int\n",
    "        img_arr = np.asarray(result_val, np.uint8) \n",
    "        \n",
    "        #用PIL转换成图片\n",
    "        img = Image.fromarray(img_arr) #可以将numpy数组转换成图片\n",
    "        img.save(result_img_path)\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为何之前设置这样的风格系数，内容系数\n",
    "1. 首先我们的style loss的计算中，gram matrix需要除以一个比较大的数tf.constant(ch * w * h, tf.float32)，导致了style loss比较小\n",
    "2. gram = tf.matmul(features, features, adjoint_a = True) \\\n",
    "            / tf.constant(ch * w * h, tf.float32)\n",
    "3. 既然style loss小，content loss大，为了均衡两者，所以一个乘500，一个乘0.1\n",
    "4. lambda_c = 0.1 #内容， lambda_s = 500 #风格\n",
    "5. style loss: 1.7 * 500 = 850\n",
    "6. content loss: 12997 * 0.1 = 1299"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 这个文件的泛化能力\n",
    "1. 首先可以通过注释和取消注释来选取不同层的特征：\n",
    "\n",
    "style_features = [\n",
    "    #vgg_for_style.conv1_2,\n",
    "    #vgg_for_style.conv2_2,\n",
    "    #vgg_for_style.conv3_3,\n",
    "    vgg_for_style.conv4_3,\n",
    "    #vgg_for_style.conv5_3,\n",
    "]\n",
    "\n",
    "2. 可以只要重建图片，或者只重建风格：\n",
    "\n",
    "lambda_c = 0 #只重建图片\n",
    "\n",
    "lambda_s = 5000 #只重建风格\n",
    "\n",
    "3. 可以为style loss 和 content loss的不同层添加系数：\n",
    "\n",
    "for c, c_ in zip(content_features, result_content_features):\n",
    "\n",
    "    content_loss += tf.reduce_mean((c - c_) ** 2, [1,2,3])\n",
    "    \n",
    "for s, s_ in zip(style_gram, result_style_gram):\n",
    "\n",
    "    style_loss += tf.reduce_mean((s - s_) ** 2, [1,2])\n",
    "    \n",
    "例如第一层的c和c_可以乘系数0.1\n",
    "\n",
    "content_loss += 0.1 * tf.reduce_mean((c - c_) ** 2, [1,2,3])\n",
    "\n",
    "例如第二层的c和c_可以乘系数0.5\n",
    "\n",
    "content_loss += 0.5 * tf.reduce_mean((c - c_) ** 2, [1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
